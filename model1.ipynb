{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15c6c75-2fed-41b8-8143-25b2e1a1fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(r'Data/train_final.csv')\n",
    "dev = pd.read_csv(r'Data/dev_final.csv')\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z0-9?.!,¿|]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert sequences to tokenizers\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    # tensor = tf.ragged.constant(tensor)\n",
    "    # print(tensor)\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(number):\n",
    "    # Create dataset (targ_lan = English, inp_lang = French)\n",
    "    inp_lang = (train['paragraph']+\" | \"+train['question']).copy()\n",
    "    targ_lang = (train['answer']).copy()\n",
    "    act_inp = []\n",
    "    act_out = []\n",
    "    ep = number\n",
    "    if number == 0:\n",
    "        ep = len(inp_lang)\n",
    "    for i in range(ep):\n",
    "        if(len(str(targ_lang[i]).split(' '))==1):\n",
    "            act_inp.append(preprocess_sentence(str(inp_lang[i])))\n",
    "            act_out.append(preprocess_sentence(str(targ_lang[i])))\n",
    "    \n",
    "    inp_lang = (dev['paragraph']+\" | \"+dev['question']).copy()\n",
    "    targ_lang = (dev['answer']).copy()\n",
    "    if number == 0:\n",
    "        ep = len(inp_lang)\n",
    "    for i in range(ep):\n",
    "        if(len(str(targ_lang[i]).split(' '))==1):\n",
    "            act_inp.append(preprocess_sentence(str(inp_lang[i])))\n",
    "            act_out.append(preprocess_sentence(str(targ_lang[i])))\n",
    "            \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(act_inp)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(act_out)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb164a16-3e1a-40d6-95bf-5f924c3eb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 579\n",
      "17116 17116 4280 4280\n"
     ]
    }
   ],
   "source": [
    "number_of_instances = 0\n",
    "\n",
    "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(number_of_instances)\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "print(max_length_targ, max_length_inp )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.42B.300d.txt', encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "inp_embedding_matrix = np.zeros((len(inp_lang_tokenizer.word_index) + 1, 300))\n",
    "targ_embedding_matrix = np.zeros((len(targ_lang_tokenizer.word_index) + 1, 300))\n",
    "\n",
    "for word, i in inp_lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        inp_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "for word, i in targ_lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        targ_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fd4434-53fb-4386-8f46-adaa8928b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 50\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8080f7-70e4-4b2e-9fe3-c6bc05dea22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 579) (1, 3)\n",
      "Encoder output shape: (batch size, sequence length, units) (1, 579, 50)\n",
      "Encoder Hidden state shape: (batch size, units) (1, 50)\n",
      "Attention result shape: (batch size, units) (1, 50)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (1, 579, 1)\n",
      "Decoder output shape: (batch_size, vocab size) (1, 9720)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Size of input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(example_input_batch.shape, example_target_batch.shape)\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,  weights=[inp_embedding_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "\n",
    "# Attention Mechanism\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "\n",
    "# Decoder class\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[targ_embedding_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "\n",
    "\n",
    "# Initialize optimizer and loss functions\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction='none')\n",
    "\n",
    "# Loss function\n",
    "def loss_function(real, pred):\n",
    "    # Take care of the padding. Not all sequences are of equal length.\n",
    "    # If there's a '0' in the sequence, the loss is being nullified\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    return\n",
    "\n",
    "def get_data(element):\n",
    "    question = ''\n",
    "    answer = ''\n",
    "    for k in element[0].numpy():\n",
    "        if k!=0:\n",
    "            question = question+ inp_lang_tokenizer.index_word[k] + ' '\n",
    "    for k in element[1].numpy():\n",
    "        if k!=0:\n",
    "            answer = answer+ targ_lang_tokenizer.index_word[k] + ' '\n",
    "    return question, answer\n",
    "\n",
    "\n",
    "import os\n",
    "checkpoint_dir = './training_checkpoints1'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf814e0-70ba-45cb-9e97-9988cd337b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 19:49:18.200621: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.4245\n",
      "Epoch 1 Batch 100 Loss 2.7458\n",
      "Epoch 1 Batch 200 Loss 1.8149\n",
      "Epoch 1 Batch 300 Loss 0.0345\n",
      "Epoch 1 Batch 400 Loss 4.9575\n",
      "Epoch 1 Batch 500 Loss 1.9590\n",
      "Epoch 1 Batch 600 Loss 2.3455\n",
      "Epoch 1 Batch 700 Loss 2.8050\n",
      "Epoch 1 Batch 800 Loss 1.1944\n",
      "Epoch 1 Batch 900 Loss 2.5513\n",
      "Epoch 1 Batch 1000 Loss 1.0732\n",
      "Epoch 1 Batch 1100 Loss 0.4388\n",
      "Epoch 1 Batch 1200 Loss 0.0794\n",
      "Epoch 1 Batch 1300 Loss 0.1432\n",
      "Epoch 1 Batch 1400 Loss 4.6689\n",
      "Epoch 1 Batch 1500 Loss 0.7886\n",
      "Epoch 1 Batch 1600 Loss 0.1795\n",
      "Epoch 1 Batch 1700 Loss 0.7464\n",
      "Epoch 1 Batch 1800 Loss 0.3038\n",
      "Epoch 1 Batch 1900 Loss 1.1206\n",
      "Epoch 1 Batch 2000 Loss 4.2130\n",
      "Epoch 1 Batch 2100 Loss 2.3105\n",
      "Epoch 1 Batch 2200 Loss 2.1616\n",
      "Epoch 1 Batch 2300 Loss 2.5812\n",
      "Epoch 1 Batch 2400 Loss 2.5249\n",
      "Epoch 1 Batch 2500 Loss 1.7243\n",
      "Epoch 1 Batch 2600 Loss 1.3915\n",
      "Epoch 1 Batch 2700 Loss 2.7068\n",
      "Epoch 1 Batch 2800 Loss 0.7097\n",
      "Epoch 1 Batch 2900 Loss 6.4095\n",
      "Epoch 1 Batch 3000 Loss 1.7551\n",
      "Epoch 1 Batch 3100 Loss 1.0452\n",
      "Epoch 1 Batch 3200 Loss 2.1643\n",
      "Epoch 1 Batch 3300 Loss 0.3222\n",
      "Epoch 1 Batch 3400 Loss 4.5246\n",
      "Epoch 1 Batch 3500 Loss 2.0086\n",
      "Epoch 1 Batch 3600 Loss 0.1633\n",
      "Epoch 1 Batch 3700 Loss 2.2706\n",
      "Epoch 1 Batch 3800 Loss 2.4376\n",
      "Epoch 1 Batch 3900 Loss 0.0987\n",
      "Epoch 1 Batch 4000 Loss 1.4574\n",
      "Epoch 1 Batch 4100 Loss 1.0154\n",
      "Epoch 1 Batch 4200 Loss 0.8807\n",
      "Epoch 1 Batch 4300 Loss 5.1862\n",
      "Epoch 1 Batch 4400 Loss 2.2765\n",
      "Epoch 1 Batch 4500 Loss 4.9981\n",
      "Epoch 1 Batch 4600 Loss 1.7984\n",
      "Epoch 1 Batch 4700 Loss 2.8320\n",
      "Epoch 1 Batch 4800 Loss 4.8451\n",
      "Epoch 1 Batch 4900 Loss 1.9430\n",
      "Epoch 1 Batch 5000 Loss 2.5207\n",
      "Epoch 1 Batch 5100 Loss 0.9610\n",
      "Epoch 1 Batch 5200 Loss 2.1727\n",
      "Epoch 1 Batch 5300 Loss 3.1298\n",
      "Epoch 1 Batch 5400 Loss 3.1560\n",
      "Epoch 1 Batch 5500 Loss 2.6365\n",
      "Epoch 1 Batch 5600 Loss 0.0330\n",
      "Epoch 1 Batch 5700 Loss 1.4345\n",
      "Epoch 1 Batch 5800 Loss 1.8612\n",
      "Epoch 1 Batch 5900 Loss 1.8266\n",
      "Epoch 1 Batch 6000 Loss 3.1785\n",
      "Epoch 1 Batch 6100 Loss 1.5841\n",
      "Epoch 1 Batch 6200 Loss 1.9580\n",
      "Epoch 1 Batch 6300 Loss 0.1055\n",
      "Epoch 1 Batch 6400 Loss 1.6968\n",
      "Epoch 1 Batch 6500 Loss 1.2054\n",
      "Epoch 1 Batch 6600 Loss 1.8448\n",
      "Epoch 1 Batch 6700 Loss 1.9260\n",
      "Epoch 1 Batch 6800 Loss 0.9614\n",
      "Epoch 1 Batch 6900 Loss 2.4070\n",
      "Epoch 1 Batch 7000 Loss 0.0335\n",
      "Epoch 1 Batch 7100 Loss 2.1024\n",
      "Epoch 1 Batch 7200 Loss 2.8330\n",
      "Epoch 1 Batch 7300 Loss 0.3473\n",
      "Epoch 1 Batch 7400 Loss 3.5837\n",
      "Epoch 1 Batch 7500 Loss 4.7098\n",
      "Epoch 1 Batch 7600 Loss 1.8870\n",
      "Epoch 1 Batch 7700 Loss 0.9388\n",
      "Epoch 1 Batch 7800 Loss 0.8218\n",
      "Epoch 1 Batch 7900 Loss 2.3340\n",
      "Epoch 1 Batch 8000 Loss 0.8780\n",
      "Epoch 1 Batch 8100 Loss 3.9934\n",
      "Epoch 1 Batch 8200 Loss 1.3681\n",
      "Epoch 1 Batch 8300 Loss 5.1043\n",
      "Epoch 1 Batch 8400 Loss 1.1918\n",
      "Epoch 1 Batch 8500 Loss 2.5828\n",
      "Epoch 1 Batch 8600 Loss 4.8863\n",
      "Epoch 1 Batch 8700 Loss 2.1786\n",
      "Epoch 1 Batch 8800 Loss 3.0454\n",
      "Epoch 1 Batch 8900 Loss 0.8658\n",
      "Epoch 1 Batch 9000 Loss 2.1775\n",
      "Epoch 1 Batch 9100 Loss 0.1498\n",
      "Epoch 1 Batch 9200 Loss 4.2899\n",
      "Epoch 1 Batch 9300 Loss 5.2075\n",
      "Epoch 1 Batch 9400 Loss 0.9039\n",
      "Epoch 1 Batch 9500 Loss 0.0318\n",
      "Epoch 1 Batch 9600 Loss 0.4527\n",
      "Epoch 1 Batch 9700 Loss 1.3141\n",
      "Epoch 1 Batch 9800 Loss 2.5195\n",
      "Epoch 1 Batch 9900 Loss 1.2619\n",
      "Epoch 1 Batch 10000 Loss 5.5552\n",
      "Epoch 1 Batch 10100 Loss 1.4758\n",
      "Epoch 1 Batch 10200 Loss 3.2033\n",
      "Epoch 1 Batch 10300 Loss 0.0633\n",
      "Epoch 1 Batch 10400 Loss 5.5744\n",
      "Epoch 1 Batch 10500 Loss 2.9963\n",
      "Epoch 1 Batch 10600 Loss 3.5281\n",
      "Epoch 1 Batch 10700 Loss 1.3743\n",
      "Epoch 1 Batch 10800 Loss 0.5379\n",
      "Epoch 1 Batch 10900 Loss 5.6585\n",
      "Epoch 1 Batch 11000 Loss 2.4053\n",
      "Epoch 1 Batch 11100 Loss 3.3489\n",
      "Epoch 1 Batch 11200 Loss 1.3770\n",
      "Epoch 1 Batch 11300 Loss 1.9402\n",
      "Epoch 1 Batch 11400 Loss 4.0267\n",
      "Epoch 1 Batch 11500 Loss 0.8257\n",
      "Epoch 1 Batch 11600 Loss 2.6013\n",
      "Epoch 1 Batch 11700 Loss 3.2253\n",
      "Epoch 1 Batch 11800 Loss 0.8934\n",
      "Epoch 1 Batch 11900 Loss 4.8684\n",
      "Epoch 1 Batch 12000 Loss 1.1963\n",
      "Epoch 1 Batch 12100 Loss 0.6888\n",
      "Epoch 1 Batch 12200 Loss 3.7588\n",
      "Epoch 1 Batch 12300 Loss 1.5117\n",
      "Epoch 1 Batch 12400 Loss 4.8999\n",
      "Epoch 1 Batch 12500 Loss 3.5621\n",
      "Epoch 1 Batch 12600 Loss 3.0972\n",
      "Epoch 1 Batch 12700 Loss 2.0800\n",
      "Epoch 1 Batch 12800 Loss 2.6220\n",
      "Epoch 1 Batch 12900 Loss 1.1506\n",
      "Epoch 1 Batch 13000 Loss 3.9808\n",
      "Epoch 1 Batch 13100 Loss 2.3758\n",
      "Epoch 1 Batch 13200 Loss 2.8223\n",
      "Epoch 1 Batch 13300 Loss 5.8873\n",
      "Epoch 1 Batch 13400 Loss 1.3110\n",
      "Epoch 1 Batch 13500 Loss 1.3248\n",
      "Epoch 1 Batch 13600 Loss 0.0571\n",
      "Epoch 1 Batch 13700 Loss 0.5700\n",
      "Epoch 1 Batch 13800 Loss 0.3357\n",
      "Epoch 1 Batch 13900 Loss 1.2154\n",
      "Epoch 1 Batch 14000 Loss 0.6629\n",
      "Epoch 1 Batch 14100 Loss 3.9786\n",
      "Epoch 1 Batch 14200 Loss 0.6751\n",
      "Epoch 1 Batch 14300 Loss 1.1932\n",
      "Epoch 1 Batch 14400 Loss 1.6755\n",
      "Epoch 1 Batch 14500 Loss 3.4153\n",
      "Epoch 1 Batch 14600 Loss 5.1146\n",
      "Epoch 1 Batch 14700 Loss 3.8965\n",
      "Epoch 1 Batch 14800 Loss 4.5402\n",
      "Epoch 1 Batch 14900 Loss 0.5206\n",
      "Epoch 1 Batch 15000 Loss 0.0790\n",
      "Epoch 1 Batch 15100 Loss 3.5662\n",
      "Epoch 1 Batch 15200 Loss 1.1717\n",
      "Epoch 1 Batch 15300 Loss 3.9611\n",
      "Epoch 1 Batch 15400 Loss 1.4606\n",
      "Epoch 1 Batch 15500 Loss 1.4434\n",
      "Epoch 1 Batch 15600 Loss 2.1871\n",
      "Epoch 1 Batch 15700 Loss 2.3564\n",
      "Epoch 1 Batch 15800 Loss 2.0013\n",
      "Epoch 1 Batch 15900 Loss 4.9138\n",
      "Epoch 1 Batch 16000 Loss 2.8623\n",
      "Epoch 1 Batch 16100 Loss 1.0185\n",
      "Epoch 1 Batch 16200 Loss 2.4670\n",
      "Epoch 1 Batch 16300 Loss 2.9703\n",
      "Epoch 1 Batch 16400 Loss 2.2343\n",
      "Epoch 1 Batch 16500 Loss 4.4164\n",
      "Epoch 1 Batch 16600 Loss 4.8325\n",
      "Epoch 1 Batch 16700 Loss 0.4702\n",
      "Epoch 1 Batch 16800 Loss 3.3452\n",
      "Epoch 1 Batch 16900 Loss 4.2789\n",
      "Epoch 1 Batch 17000 Loss 0.0494\n",
      "Epoch 1 Batch 17100 Loss 4.2076\n",
      "Epoch 1 Loss 2.1286\n",
      "Time taken for 1 epoch 781.0025699138641 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.2436\n",
      "Epoch 2 Batch 100 Loss 2.4017\n",
      "Epoch 2 Batch 200 Loss 3.2673\n",
      "Epoch 2 Batch 300 Loss 2.5532\n",
      "Epoch 2 Batch 400 Loss 2.5627\n",
      "Epoch 2 Batch 500 Loss 1.4459\n",
      "Epoch 2 Batch 600 Loss 1.2430\n",
      "Epoch 2 Batch 700 Loss 1.8735\n",
      "Epoch 2 Batch 800 Loss 3.2258\n",
      "Epoch 2 Batch 900 Loss 0.5500\n",
      "Epoch 2 Batch 1000 Loss 0.2123\n",
      "Epoch 2 Batch 1100 Loss 1.3400\n",
      "Epoch 2 Batch 1200 Loss 1.7066\n",
      "Epoch 2 Batch 1300 Loss 0.0684\n",
      "Epoch 2 Batch 1400 Loss 1.4489\n",
      "Epoch 2 Batch 1500 Loss 4.0228\n",
      "Epoch 2 Batch 1600 Loss 0.8538\n",
      "Epoch 2 Batch 1700 Loss 2.2502\n",
      "Epoch 2 Batch 1800 Loss 0.4780\n",
      "Epoch 2 Batch 1900 Loss 0.3291\n",
      "Epoch 2 Batch 2000 Loss 2.0481\n",
      "Epoch 2 Batch 2100 Loss 0.4854\n",
      "Epoch 2 Batch 2200 Loss 0.8672\n",
      "Epoch 2 Batch 2300 Loss 2.4950\n",
      "Epoch 2 Batch 2400 Loss 0.6168\n",
      "Epoch 2 Batch 2500 Loss 0.3543\n",
      "Epoch 2 Batch 2600 Loss 2.3282\n",
      "Epoch 2 Batch 2700 Loss 1.7208\n",
      "Epoch 2 Batch 2800 Loss 0.9365\n",
      "Epoch 2 Batch 2900 Loss 1.9500\n",
      "Epoch 2 Batch 3000 Loss 0.9224\n",
      "Epoch 2 Batch 3100 Loss 1.1643\n",
      "Epoch 2 Batch 3200 Loss 1.3915\n",
      "Epoch 2 Batch 3300 Loss 0.0718\n",
      "Epoch 2 Batch 3400 Loss 0.8158\n",
      "Epoch 2 Batch 3500 Loss 3.3498\n",
      "Epoch 2 Batch 3600 Loss 0.6275\n",
      "Epoch 2 Batch 3700 Loss 0.0171\n",
      "Epoch 2 Batch 3800 Loss 1.1630\n",
      "Epoch 2 Batch 3900 Loss 1.6027\n",
      "Epoch 2 Batch 4000 Loss 0.2589\n",
      "Epoch 2 Batch 4100 Loss 2.3992\n",
      "Epoch 2 Batch 4200 Loss 1.0534\n",
      "Epoch 2 Batch 4300 Loss 0.1975\n",
      "Epoch 2 Batch 4400 Loss 1.0008\n",
      "Epoch 2 Batch 4500 Loss 1.8178\n",
      "Epoch 2 Batch 4600 Loss 0.0142\n",
      "Epoch 2 Batch 4700 Loss 0.1059\n",
      "Epoch 2 Batch 4800 Loss 0.3577\n",
      "Epoch 2 Batch 4900 Loss 3.5213\n",
      "Epoch 2 Batch 5000 Loss 0.1851\n",
      "Epoch 2 Batch 5100 Loss 0.3759\n",
      "Epoch 2 Batch 5200 Loss 2.9401\n",
      "Epoch 2 Batch 5300 Loss 0.0989\n",
      "Epoch 2 Batch 5400 Loss 1.6704\n",
      "Epoch 2 Batch 5500 Loss 0.8791\n",
      "Epoch 2 Batch 5600 Loss 0.0324\n",
      "Epoch 2 Batch 5700 Loss 0.6868\n",
      "Epoch 2 Batch 5800 Loss 1.4702\n",
      "Epoch 2 Batch 5900 Loss 2.4555\n",
      "Epoch 2 Batch 6000 Loss 1.6540\n",
      "Epoch 2 Batch 6100 Loss 0.0100\n",
      "Epoch 2 Batch 6200 Loss 2.1891\n",
      "Epoch 2 Batch 6300 Loss 1.5526\n",
      "Epoch 2 Batch 6400 Loss 2.9580\n",
      "Epoch 2 Batch 6500 Loss 1.3728\n",
      "Epoch 2 Batch 6600 Loss 2.8788\n",
      "Epoch 2 Batch 6700 Loss 0.9212\n",
      "Epoch 2 Batch 6800 Loss 2.7331\n",
      "Epoch 2 Batch 6900 Loss 4.7906\n",
      "Epoch 2 Batch 7000 Loss 0.2205\n",
      "Epoch 2 Batch 7100 Loss 1.3780\n",
      "Epoch 2 Batch 7200 Loss 1.2945\n",
      "Epoch 2 Batch 7300 Loss 3.2666\n",
      "Epoch 2 Batch 7400 Loss 2.4475\n",
      "Epoch 2 Batch 7500 Loss 2.9196\n",
      "Epoch 2 Batch 7600 Loss 2.3895\n",
      "Epoch 2 Batch 7700 Loss 0.5617\n",
      "Epoch 2 Batch 7800 Loss 0.1007\n",
      "Epoch 2 Batch 7900 Loss 1.3676\n",
      "Epoch 2 Batch 8000 Loss 0.0331\n",
      "Epoch 2 Batch 8100 Loss 2.9785\n",
      "Epoch 2 Batch 8200 Loss 0.0544\n",
      "Epoch 2 Batch 8300 Loss 1.2908\n",
      "Epoch 2 Batch 8400 Loss 0.6064\n",
      "Epoch 2 Batch 8500 Loss 0.0356\n",
      "Epoch 2 Batch 8600 Loss 1.8980\n",
      "Epoch 2 Batch 8700 Loss 1.4671\n",
      "Epoch 2 Batch 8800 Loss 2.2868\n",
      "Epoch 2 Batch 8900 Loss 1.4429\n",
      "Epoch 2 Batch 9000 Loss 2.8841\n",
      "Epoch 2 Batch 9100 Loss 4.1878\n",
      "Epoch 2 Batch 9200 Loss 3.2682\n",
      "Epoch 2 Batch 9300 Loss 3.2937\n",
      "Epoch 2 Batch 9400 Loss 2.7066\n",
      "Epoch 2 Batch 9500 Loss 2.3899\n",
      "Epoch 2 Batch 9600 Loss 0.5756\n",
      "Epoch 2 Batch 9700 Loss 5.0096\n",
      "Epoch 2 Batch 9800 Loss 1.3811\n",
      "Epoch 2 Batch 9900 Loss 0.9997\n",
      "Epoch 2 Batch 10000 Loss 1.5252\n",
      "Epoch 2 Batch 10100 Loss 5.0605\n",
      "Epoch 2 Batch 10200 Loss 3.3357\n",
      "Epoch 2 Batch 10300 Loss 3.7485\n",
      "Epoch 2 Batch 10400 Loss 4.1743\n",
      "Epoch 2 Batch 10500 Loss 4.7937\n",
      "Epoch 2 Batch 10600 Loss 1.0649\n",
      "Epoch 2 Batch 10700 Loss 0.3927\n",
      "Epoch 2 Batch 10800 Loss 4.3189\n",
      "Epoch 2 Batch 10900 Loss 2.3594\n",
      "Epoch 2 Batch 11000 Loss 2.6926\n",
      "Epoch 2 Batch 11100 Loss 0.0963\n",
      "Epoch 2 Batch 11200 Loss 0.8201\n",
      "Epoch 2 Batch 11300 Loss 4.7980\n",
      "Epoch 2 Batch 11400 Loss 4.3806\n",
      "Epoch 2 Batch 11500 Loss 0.6257\n",
      "Epoch 2 Batch 11600 Loss 1.5187\n",
      "Epoch 2 Batch 11700 Loss 0.8963\n",
      "Epoch 2 Batch 11800 Loss 0.3206\n",
      "Epoch 2 Batch 11900 Loss 1.6947\n",
      "Epoch 2 Batch 12000 Loss 3.8653\n",
      "Epoch 2 Batch 12100 Loss 1.2557\n",
      "Epoch 2 Batch 12200 Loss 1.4217\n",
      "Epoch 2 Batch 12300 Loss 0.6743\n",
      "Epoch 2 Batch 12400 Loss 1.8594\n",
      "Epoch 2 Batch 12500 Loss 3.3916\n",
      "Epoch 2 Batch 12600 Loss 5.9671\n",
      "Epoch 2 Batch 12700 Loss 2.2711\n",
      "Epoch 2 Batch 12800 Loss 0.2505\n",
      "Epoch 2 Batch 12900 Loss 3.2573\n",
      "Epoch 2 Batch 13000 Loss 2.7532\n",
      "Epoch 2 Batch 13100 Loss 2.1675\n",
      "Epoch 2 Batch 13200 Loss 1.0209\n",
      "Epoch 2 Batch 13300 Loss 2.0923\n",
      "Epoch 2 Batch 13400 Loss 4.5865\n",
      "Epoch 2 Batch 13500 Loss 3.4123\n",
      "Epoch 2 Batch 13600 Loss 1.1442\n",
      "Epoch 2 Batch 13700 Loss 4.9413\n",
      "Epoch 2 Batch 13800 Loss 2.1830\n",
      "Epoch 2 Batch 13900 Loss 3.3029\n",
      "Epoch 2 Batch 14000 Loss 1.5299\n",
      "Epoch 2 Batch 14100 Loss 0.9974\n",
      "Epoch 2 Batch 14200 Loss 0.5582\n",
      "Epoch 2 Batch 14300 Loss 0.7997\n",
      "Epoch 2 Batch 14400 Loss 0.9443\n",
      "Epoch 2 Batch 14500 Loss 2.2541\n",
      "Epoch 2 Batch 14600 Loss 2.6983\n",
      "Epoch 2 Batch 14700 Loss 0.0431\n",
      "Epoch 2 Batch 14800 Loss 2.1962\n",
      "Epoch 2 Batch 14900 Loss 2.6290\n",
      "Epoch 2 Batch 15000 Loss 4.1158\n",
      "Epoch 2 Batch 15100 Loss 4.0919\n",
      "Epoch 2 Batch 15200 Loss 3.9458\n",
      "Epoch 2 Batch 15300 Loss 1.2537\n",
      "Epoch 2 Batch 15400 Loss 3.5354\n",
      "Epoch 2 Batch 15500 Loss 2.5610\n",
      "Epoch 2 Batch 15600 Loss 2.9510\n",
      "Epoch 2 Batch 15700 Loss 2.6125\n",
      "Epoch 2 Batch 15800 Loss 4.3803\n",
      "Epoch 2 Batch 15900 Loss 2.6951\n",
      "Epoch 2 Batch 16000 Loss 3.7070\n",
      "Epoch 2 Batch 16100 Loss 3.2836\n",
      "Epoch 2 Batch 16200 Loss 2.3156\n",
      "Epoch 2 Batch 16300 Loss 1.5947\n",
      "Epoch 2 Batch 16400 Loss 4.1965\n",
      "Epoch 2 Batch 16500 Loss 0.8915\n",
      "Epoch 2 Batch 16600 Loss 3.1413\n",
      "Epoch 2 Batch 16700 Loss 4.3191\n",
      "Epoch 2 Batch 16800 Loss 1.5130\n",
      "Epoch 2 Batch 16900 Loss 0.8713\n",
      "Epoch 2 Batch 17000 Loss 3.6261\n",
      "Epoch 2 Batch 17100 Loss 4.3922\n",
      "Epoch 2 Loss 1.9461\n",
      "Time taken for 1 epoch 852.2604417800903 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.4153\n",
      "Epoch 3 Batch 100 Loss 2.2587\n",
      "Epoch 3 Batch 200 Loss 1.4134\n",
      "Epoch 3 Batch 300 Loss 0.5824\n",
      "Epoch 3 Batch 400 Loss 3.8875\n",
      "Epoch 3 Batch 500 Loss 0.1205\n",
      "Epoch 3 Batch 600 Loss 0.4653\n",
      "Epoch 3 Batch 700 Loss 0.7986\n",
      "Epoch 3 Batch 800 Loss 0.0130\n",
      "Epoch 3 Batch 900 Loss 0.6122\n",
      "Epoch 3 Batch 1000 Loss 1.5577\n",
      "Epoch 3 Batch 1100 Loss 3.0808\n",
      "Epoch 3 Batch 1200 Loss 3.5427\n",
      "Epoch 3 Batch 1300 Loss 1.5427\n",
      "Epoch 3 Batch 1400 Loss 0.1917\n",
      "Epoch 3 Batch 1500 Loss 0.6791\n",
      "Epoch 3 Batch 1600 Loss 1.5871\n",
      "Epoch 3 Batch 1700 Loss 2.0195\n",
      "Epoch 3 Batch 1800 Loss 0.1499\n",
      "Epoch 3 Batch 1900 Loss 2.4679\n",
      "Epoch 3 Batch 2000 Loss 0.6354\n",
      "Epoch 3 Batch 2100 Loss 2.8765\n",
      "Epoch 3 Batch 2200 Loss 0.2637\n",
      "Epoch 3 Batch 2300 Loss 1.6117\n",
      "Epoch 3 Batch 2400 Loss 2.8198\n",
      "Epoch 3 Batch 2500 Loss 2.0394\n",
      "Epoch 3 Batch 2600 Loss 2.1526\n",
      "Epoch 3 Batch 2700 Loss 2.7477\n",
      "Epoch 3 Batch 2800 Loss 2.0706\n",
      "Epoch 3 Batch 2900 Loss 2.5047\n",
      "Epoch 3 Batch 3000 Loss 1.8937\n",
      "Epoch 3 Batch 3100 Loss 2.7889\n",
      "Epoch 3 Batch 3200 Loss 1.3256\n",
      "Epoch 3 Batch 3300 Loss 1.5112\n",
      "Epoch 3 Batch 3400 Loss 0.9053\n",
      "Epoch 3 Batch 3500 Loss 1.9485\n",
      "Epoch 3 Batch 3600 Loss 1.1248\n",
      "Epoch 3 Batch 3700 Loss 2.2731\n",
      "Epoch 3 Batch 3800 Loss 2.5530\n",
      "Epoch 3 Batch 3900 Loss 0.6129\n",
      "Epoch 3 Batch 4000 Loss 0.5395\n",
      "Epoch 3 Batch 4100 Loss 2.8132\n",
      "Epoch 3 Batch 4200 Loss 2.8050\n",
      "Epoch 3 Batch 4300 Loss 1.1712\n",
      "Epoch 3 Batch 4400 Loss 2.7893\n",
      "Epoch 3 Batch 4500 Loss 2.4382\n",
      "Epoch 3 Batch 4600 Loss 3.9565\n",
      "Epoch 3 Batch 4700 Loss 2.4181\n",
      "Epoch 3 Batch 4800 Loss 0.1747\n",
      "Epoch 3 Batch 4900 Loss 2.5199\n",
      "Epoch 3 Batch 5000 Loss 2.2633\n",
      "Epoch 3 Batch 5100 Loss 1.0378\n",
      "Epoch 3 Batch 5200 Loss 2.4475\n",
      "Epoch 3 Batch 5300 Loss 0.5825\n",
      "Epoch 3 Batch 5400 Loss 0.8463\n",
      "Epoch 3 Batch 5500 Loss 3.7705\n",
      "Epoch 3 Batch 5600 Loss 0.3795\n",
      "Epoch 3 Batch 5700 Loss 1.2444\n",
      "Epoch 3 Batch 5800 Loss 1.1568\n",
      "Epoch 3 Batch 5900 Loss 0.5206\n",
      "Epoch 3 Batch 6000 Loss 3.5167\n",
      "Epoch 3 Batch 6100 Loss 0.4959\n",
      "Epoch 3 Batch 6200 Loss 3.4512\n",
      "Epoch 3 Batch 6300 Loss 0.0784\n",
      "Epoch 3 Batch 6400 Loss 1.3361\n",
      "Epoch 3 Batch 6500 Loss 1.2590\n",
      "Epoch 3 Batch 6600 Loss 0.0077\n",
      "Epoch 3 Batch 6700 Loss 3.6404\n",
      "Epoch 3 Batch 6800 Loss 3.0222\n",
      "Epoch 3 Batch 6900 Loss 3.0533\n",
      "Epoch 3 Batch 7000 Loss 3.4160\n",
      "Epoch 3 Batch 7100 Loss 1.5831\n",
      "Epoch 3 Batch 7200 Loss 3.1409\n",
      "Epoch 3 Batch 7300 Loss 3.0569\n",
      "Epoch 3 Batch 7400 Loss 0.4827\n",
      "Epoch 3 Batch 7500 Loss 0.3958\n",
      "Epoch 3 Batch 7600 Loss 3.1697\n",
      "Epoch 3 Batch 7700 Loss 1.0202\n",
      "Epoch 3 Batch 7800 Loss 0.1059\n",
      "Epoch 3 Batch 7900 Loss 2.7417\n",
      "Epoch 3 Batch 8000 Loss 1.8694\n",
      "Epoch 3 Batch 8100 Loss 0.2368\n",
      "Epoch 3 Batch 8200 Loss 0.2216\n",
      "Epoch 3 Batch 8300 Loss 0.4935\n",
      "Epoch 3 Batch 8400 Loss 0.6552\n",
      "Epoch 3 Batch 8500 Loss 1.9524\n",
      "Epoch 3 Batch 8600 Loss 0.9044\n",
      "Epoch 3 Batch 8700 Loss 0.7531\n",
      "Epoch 3 Batch 8800 Loss 2.7139\n",
      "Epoch 3 Batch 8900 Loss 1.5500\n",
      "Epoch 3 Batch 9000 Loss 1.1334\n",
      "Epoch 3 Batch 9100 Loss 0.8824\n",
      "Epoch 3 Batch 9200 Loss 0.9783\n",
      "Epoch 3 Batch 9300 Loss 1.6096\n",
      "Epoch 3 Batch 9400 Loss 3.9774\n",
      "Epoch 3 Batch 9500 Loss 3.4901\n",
      "Epoch 3 Batch 9600 Loss 3.8129\n",
      "Epoch 3 Batch 9700 Loss 3.2557\n",
      "Epoch 3 Batch 9800 Loss 3.9860\n",
      "Epoch 3 Batch 9900 Loss 5.6204\n",
      "Epoch 3 Batch 10000 Loss 0.3268\n",
      "Epoch 3 Batch 10100 Loss 3.7556\n",
      "Epoch 3 Batch 10200 Loss 0.0307\n",
      "Epoch 3 Batch 10300 Loss 0.8995\n",
      "Epoch 3 Batch 10400 Loss 1.1585\n",
      "Epoch 3 Batch 10500 Loss 2.4320\n",
      "Epoch 3 Batch 10600 Loss 1.4703\n",
      "Epoch 3 Batch 10700 Loss 0.4221\n",
      "Epoch 3 Batch 10800 Loss 0.4713\n",
      "Epoch 3 Batch 10900 Loss 1.2733\n",
      "Epoch 3 Batch 11000 Loss 2.2851\n",
      "Epoch 3 Batch 11100 Loss 2.4801\n",
      "Epoch 3 Batch 11200 Loss 2.5045\n",
      "Epoch 3 Batch 11300 Loss 3.5119\n",
      "Epoch 3 Batch 11400 Loss 2.9013\n",
      "Epoch 3 Batch 11500 Loss 0.7332\n",
      "Epoch 3 Batch 11600 Loss 3.8043\n",
      "Epoch 3 Batch 11700 Loss 1.8824\n",
      "Epoch 3 Batch 11800 Loss 1.8349\n",
      "Epoch 3 Batch 11900 Loss 1.1853\n",
      "Epoch 3 Batch 12000 Loss 1.0274\n",
      "Epoch 3 Batch 12100 Loss 3.8513\n",
      "Epoch 3 Batch 12200 Loss 4.6140\n",
      "Epoch 3 Batch 12300 Loss 3.2814\n",
      "Epoch 3 Batch 12400 Loss 1.9067\n",
      "Epoch 3 Batch 12500 Loss 1.1088\n",
      "Epoch 3 Batch 12600 Loss 3.1834\n",
      "Epoch 3 Batch 12700 Loss 2.8122\n",
      "Epoch 3 Batch 12800 Loss 0.1357\n",
      "Epoch 3 Batch 12900 Loss 2.2415\n",
      "Epoch 3 Batch 13000 Loss 2.8798\n",
      "Epoch 3 Batch 13100 Loss 0.8069\n",
      "Epoch 3 Batch 13200 Loss 0.0946\n",
      "Epoch 3 Batch 13300 Loss 4.0799\n",
      "Epoch 3 Batch 13400 Loss 4.1232\n",
      "Epoch 3 Batch 13500 Loss 2.7815\n",
      "Epoch 3 Batch 13600 Loss 2.2378\n",
      "Epoch 3 Batch 13700 Loss 1.9120\n",
      "Epoch 3 Batch 13800 Loss 0.8259\n",
      "Epoch 3 Batch 13900 Loss 1.7070\n",
      "Epoch 3 Batch 14000 Loss 3.7621\n",
      "Epoch 3 Batch 14100 Loss 3.2014\n",
      "Epoch 3 Batch 14200 Loss 3.4921\n",
      "Epoch 3 Batch 14300 Loss 2.7846\n",
      "Epoch 3 Batch 14400 Loss 3.6261\n",
      "Epoch 3 Batch 14500 Loss 1.4243\n",
      "Epoch 3 Batch 14600 Loss 1.2302\n",
      "Epoch 3 Batch 14700 Loss 2.1230\n",
      "Epoch 3 Batch 14800 Loss 0.0619\n",
      "Epoch 3 Batch 14900 Loss 0.3344\n",
      "Epoch 3 Batch 15000 Loss 3.0905\n",
      "Epoch 3 Batch 15100 Loss 3.2694\n",
      "Epoch 3 Batch 15200 Loss 1.7941\n",
      "Epoch 3 Batch 15300 Loss 3.3856\n",
      "Epoch 3 Batch 15400 Loss 2.4785\n",
      "Epoch 3 Batch 15500 Loss 4.0349\n",
      "Epoch 3 Batch 15600 Loss 2.0266\n",
      "Epoch 3 Batch 15700 Loss 0.8524\n",
      "Epoch 3 Batch 15800 Loss 3.5304\n",
      "Epoch 3 Batch 15900 Loss 1.9837\n",
      "Epoch 3 Batch 16000 Loss 1.4126\n",
      "Epoch 3 Batch 16100 Loss 1.2429\n",
      "Epoch 3 Batch 16200 Loss 0.7236\n",
      "Epoch 3 Batch 16300 Loss 0.3052\n",
      "Epoch 3 Batch 16400 Loss 2.4709\n",
      "Epoch 3 Batch 16500 Loss 3.4442\n",
      "Epoch 3 Batch 16600 Loss 5.5723\n",
      "Epoch 3 Batch 16700 Loss 2.2102\n",
      "Epoch 3 Batch 16800 Loss 5.5432\n",
      "Epoch 3 Batch 16900 Loss 2.1993\n",
      "Epoch 3 Batch 17000 Loss 0.5928\n",
      "Epoch 3 Batch 17100 Loss 1.0135\n",
      "Epoch 3 Loss 1.8483\n",
      "Time taken for 1 epoch 766.8917987346649 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0895\n",
      "Epoch 4 Batch 100 Loss 0.9096\n",
      "Epoch 4 Batch 200 Loss 1.7703\n",
      "Epoch 4 Batch 300 Loss 1.5388\n",
      "Epoch 4 Batch 400 Loss 0.3647\n",
      "Epoch 4 Batch 500 Loss 1.4513\n",
      "Epoch 4 Batch 600 Loss 0.3566\n",
      "Epoch 4 Batch 700 Loss 1.1530\n",
      "Epoch 4 Batch 800 Loss 2.0902\n",
      "Epoch 4 Batch 900 Loss 2.4048\n",
      "Epoch 4 Batch 1000 Loss 1.4303\n",
      "Epoch 4 Batch 1100 Loss 0.0602\n",
      "Epoch 4 Batch 1200 Loss 2.5859\n",
      "Epoch 4 Batch 1300 Loss 0.3061\n",
      "Epoch 4 Batch 1400 Loss 2.2818\n",
      "Epoch 4 Batch 1500 Loss 0.7162\n",
      "Epoch 4 Batch 1600 Loss 3.4523\n",
      "Epoch 4 Batch 1700 Loss 2.5481\n",
      "Epoch 4 Batch 1800 Loss 0.8156\n",
      "Epoch 4 Batch 1900 Loss 0.6244\n",
      "Epoch 4 Batch 2000 Loss 0.7426\n",
      "Epoch 4 Batch 2100 Loss 0.1140\n",
      "Epoch 4 Batch 2200 Loss 0.4312\n",
      "Epoch 4 Batch 2300 Loss 2.4991\n",
      "Epoch 4 Batch 2400 Loss 0.4404\n",
      "Epoch 4 Batch 2500 Loss 2.3362\n",
      "Epoch 4 Batch 2600 Loss 1.1968\n",
      "Epoch 4 Batch 2700 Loss 1.7166\n",
      "Epoch 4 Batch 2800 Loss 1.8028\n",
      "Epoch 4 Batch 2900 Loss 0.9844\n",
      "Epoch 4 Batch 3000 Loss 2.4040\n",
      "Epoch 4 Batch 3100 Loss 0.2379\n",
      "Epoch 4 Batch 3200 Loss 0.8058\n",
      "Epoch 4 Batch 3300 Loss 0.8294\n",
      "Epoch 4 Batch 3400 Loss 0.1329\n",
      "Epoch 4 Batch 3500 Loss 2.8804\n",
      "Epoch 4 Batch 3600 Loss 1.3131\n",
      "Epoch 4 Batch 3700 Loss 1.6554\n",
      "Epoch 4 Batch 3800 Loss 1.6872\n",
      "Epoch 4 Batch 3900 Loss 0.5624\n",
      "Epoch 4 Batch 4000 Loss 0.1903\n",
      "Epoch 4 Batch 4100 Loss 2.2836\n",
      "Epoch 4 Batch 4200 Loss 2.6312\n",
      "Epoch 4 Batch 4300 Loss 0.4611\n",
      "Epoch 4 Batch 4400 Loss 1.8597\n",
      "Epoch 4 Batch 4500 Loss 1.1216\n",
      "Epoch 4 Batch 4600 Loss 3.6923\n",
      "Epoch 4 Batch 4700 Loss 3.8396\n",
      "Epoch 4 Batch 4800 Loss 4.0469\n",
      "Epoch 4 Batch 4900 Loss 1.9714\n",
      "Epoch 4 Batch 5000 Loss 2.8519\n",
      "Epoch 4 Batch 5100 Loss 2.2467\n",
      "Epoch 4 Batch 5200 Loss 2.1943\n",
      "Epoch 4 Batch 5300 Loss 3.3473\n",
      "Epoch 4 Batch 5400 Loss 0.2823\n",
      "Epoch 4 Batch 5500 Loss 3.0665\n",
      "Epoch 4 Batch 5600 Loss 2.2031\n",
      "Epoch 4 Batch 5700 Loss 3.5059\n",
      "Epoch 4 Batch 5800 Loss 0.5005\n",
      "Epoch 4 Batch 5900 Loss 0.3619\n",
      "Epoch 4 Batch 6000 Loss 3.3345\n",
      "Epoch 4 Batch 6100 Loss 0.5246\n",
      "Epoch 4 Batch 6200 Loss 1.8985\n",
      "Epoch 4 Batch 6300 Loss 1.1310\n",
      "Epoch 4 Batch 6400 Loss 0.4929\n",
      "Epoch 4 Batch 6500 Loss 0.4511\n",
      "Epoch 4 Batch 6600 Loss 1.3747\n",
      "Epoch 4 Batch 6700 Loss 2.7461\n",
      "Epoch 4 Batch 6800 Loss 2.7391\n",
      "Epoch 4 Batch 6900 Loss 1.9582\n",
      "Epoch 4 Batch 7000 Loss 4.1874\n",
      "Epoch 4 Batch 7100 Loss 2.8905\n",
      "Epoch 4 Batch 7200 Loss 0.4384\n",
      "Epoch 4 Batch 7300 Loss 2.0448\n",
      "Epoch 4 Batch 7400 Loss 0.0686\n",
      "Epoch 4 Batch 7500 Loss 3.4007\n",
      "Epoch 4 Batch 7600 Loss 2.0546\n",
      "Epoch 4 Batch 7700 Loss 0.5023\n",
      "Epoch 4 Batch 7800 Loss 0.9473\n",
      "Epoch 4 Batch 7900 Loss 1.2345\n",
      "Epoch 4 Batch 8000 Loss 2.6903\n",
      "Epoch 4 Batch 8100 Loss 1.8164\n",
      "Epoch 4 Batch 8200 Loss 4.6023\n",
      "Epoch 4 Batch 8300 Loss 2.3239\n",
      "Epoch 4 Batch 8400 Loss 0.1867\n",
      "Epoch 4 Batch 8500 Loss 0.8673\n",
      "Epoch 4 Batch 8600 Loss 1.0135\n",
      "Epoch 4 Batch 8700 Loss 2.4633\n",
      "Epoch 4 Batch 8800 Loss 0.5184\n",
      "Epoch 4 Batch 8900 Loss 2.3958\n",
      "Epoch 4 Batch 9000 Loss 2.7618\n",
      "Epoch 4 Batch 9100 Loss 0.4002\n",
      "Epoch 4 Batch 9200 Loss 1.5155\n",
      "Epoch 4 Batch 9300 Loss 1.6694\n",
      "Epoch 4 Batch 9400 Loss 1.1942\n",
      "Epoch 4 Batch 9500 Loss 0.6955\n",
      "Epoch 4 Batch 9600 Loss 0.9415\n",
      "Epoch 4 Batch 9700 Loss 2.1125\n",
      "Epoch 4 Batch 9800 Loss 3.5585\n",
      "Epoch 4 Batch 9900 Loss 2.6251\n",
      "Epoch 4 Batch 10000 Loss 1.2719\n",
      "Epoch 4 Batch 10100 Loss 4.1683\n",
      "Epoch 4 Batch 10200 Loss 3.3830\n",
      "Epoch 4 Batch 10300 Loss 1.8177\n",
      "Epoch 4 Batch 10400 Loss 0.0601\n",
      "Epoch 4 Batch 10500 Loss 2.5285\n",
      "Epoch 4 Batch 10600 Loss 1.8111\n",
      "Epoch 4 Batch 10700 Loss 2.0837\n",
      "Epoch 4 Batch 10800 Loss 2.0477\n",
      "Epoch 4 Batch 10900 Loss 1.3938\n",
      "Epoch 4 Batch 11000 Loss 1.0600\n",
      "Epoch 4 Batch 11100 Loss 1.7614\n",
      "Epoch 4 Batch 11200 Loss 0.3782\n",
      "Epoch 4 Batch 11300 Loss 0.0153\n",
      "Epoch 4 Batch 11400 Loss 0.0041\n",
      "Epoch 4 Batch 11500 Loss 0.0241\n",
      "Epoch 4 Batch 11600 Loss 4.2775\n",
      "Epoch 4 Batch 11700 Loss 2.6704\n",
      "Epoch 4 Batch 11800 Loss 0.2599\n",
      "Epoch 4 Batch 11900 Loss 2.0043\n",
      "Epoch 4 Batch 12000 Loss 2.3718\n",
      "Epoch 4 Batch 12100 Loss 2.4101\n",
      "Epoch 4 Batch 12200 Loss 0.2338\n",
      "Epoch 4 Batch 12300 Loss 2.9028\n",
      "Epoch 4 Batch 12400 Loss 0.2736\n",
      "Epoch 4 Batch 12500 Loss 3.0422\n",
      "Epoch 4 Batch 12600 Loss 1.5800\n",
      "Epoch 4 Batch 12700 Loss 1.0807\n",
      "Epoch 4 Batch 12800 Loss 0.3677\n",
      "Epoch 4 Batch 12900 Loss 2.1191\n",
      "Epoch 4 Batch 13000 Loss 2.3299\n",
      "Epoch 4 Batch 13100 Loss 0.7579\n",
      "Epoch 4 Batch 13200 Loss 1.4666\n",
      "Epoch 4 Batch 13300 Loss 2.9356\n",
      "Epoch 4 Batch 13400 Loss 2.0571\n",
      "Epoch 4 Batch 13500 Loss 0.4155\n",
      "Epoch 4 Batch 13600 Loss 4.3127\n",
      "Epoch 4 Batch 13700 Loss 2.5326\n",
      "Epoch 4 Batch 13800 Loss 2.7731\n",
      "Epoch 4 Batch 13900 Loss 4.8811\n",
      "Epoch 4 Batch 14000 Loss 0.3563\n",
      "Epoch 4 Batch 14100 Loss 4.6097\n",
      "Epoch 4 Batch 14200 Loss 0.5991\n",
      "Epoch 4 Batch 14300 Loss 4.9957\n",
      "Epoch 4 Batch 14400 Loss 1.3829\n",
      "Epoch 4 Batch 14500 Loss 0.9199\n",
      "Epoch 4 Batch 14600 Loss 1.7958\n",
      "Epoch 4 Batch 14700 Loss 2.9413\n",
      "Epoch 4 Batch 14800 Loss 3.2643\n",
      "Epoch 4 Batch 14900 Loss 0.9975\n",
      "Epoch 4 Batch 15000 Loss 2.8326\n",
      "Epoch 4 Batch 15100 Loss 1.2246\n",
      "Epoch 4 Batch 15200 Loss 1.5218\n",
      "Epoch 4 Batch 15300 Loss 0.1860\n",
      "Epoch 4 Batch 15400 Loss 1.2176\n",
      "Epoch 4 Batch 15500 Loss 1.6908\n",
      "Epoch 4 Batch 15600 Loss 1.4024\n",
      "Epoch 4 Batch 15700 Loss 3.3541\n",
      "Epoch 4 Batch 15800 Loss 2.5282\n",
      "Epoch 4 Batch 15900 Loss 1.0345\n",
      "Epoch 4 Batch 16000 Loss 4.1516\n",
      "Epoch 4 Batch 16100 Loss 0.2811\n",
      "Epoch 4 Batch 16200 Loss 4.3361\n",
      "Epoch 4 Batch 16300 Loss 2.1468\n",
      "Epoch 4 Batch 16400 Loss 4.7046\n",
      "Epoch 4 Batch 16500 Loss 4.7009\n",
      "Epoch 4 Batch 16600 Loss 0.1859\n",
      "Epoch 4 Batch 16700 Loss 4.3327\n",
      "Epoch 4 Batch 16800 Loss 4.2140\n",
      "Epoch 4 Batch 16900 Loss 4.6203\n",
      "Epoch 4 Batch 17000 Loss 3.8263\n",
      "Epoch 4 Batch 17100 Loss 2.5225\n",
      "Epoch 4 Loss 1.7992\n",
      "Time taken for 1 epoch 934.8094642162323 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9248\n",
      "Epoch 5 Batch 100 Loss 1.5406\n",
      "Epoch 5 Batch 200 Loss 2.1394\n",
      "Epoch 5 Batch 300 Loss 1.0294\n",
      "Epoch 5 Batch 400 Loss 1.3390\n",
      "Epoch 5 Batch 500 Loss 1.2734\n",
      "Epoch 5 Batch 600 Loss 1.5313\n",
      "Epoch 5 Batch 700 Loss 2.2568\n",
      "Epoch 5 Batch 800 Loss 2.8996\n",
      "Epoch 5 Batch 900 Loss 1.3708\n",
      "Epoch 5 Batch 1000 Loss 0.6998\n",
      "Epoch 5 Batch 1100 Loss 1.3949\n",
      "Epoch 5 Batch 1200 Loss 0.0409\n",
      "Epoch 5 Batch 1300 Loss 0.0464\n",
      "Epoch 5 Batch 1400 Loss 0.8674\n",
      "Epoch 5 Batch 1500 Loss 0.3250\n",
      "Epoch 5 Batch 1600 Loss 2.3222\n",
      "Epoch 5 Batch 1700 Loss 2.4137\n",
      "Epoch 5 Batch 1800 Loss 2.4541\n",
      "Epoch 5 Batch 1900 Loss 0.0154\n",
      "Epoch 5 Batch 2000 Loss 2.2552\n",
      "Epoch 5 Batch 2100 Loss 1.6929\n",
      "Epoch 5 Batch 2200 Loss 0.5081\n",
      "Epoch 5 Batch 2300 Loss 0.0796\n",
      "Epoch 5 Batch 2400 Loss 1.1619\n",
      "Epoch 5 Batch 2500 Loss 0.2993\n",
      "Epoch 5 Batch 2600 Loss 0.9915\n",
      "Epoch 5 Batch 2700 Loss 0.1745\n",
      "Epoch 5 Batch 2800 Loss 0.3444\n",
      "Epoch 5 Batch 2900 Loss 1.4222\n",
      "Epoch 5 Batch 3000 Loss 0.0161\n",
      "Epoch 5 Batch 3100 Loss 3.2102\n",
      "Epoch 5 Batch 3200 Loss 0.0758\n",
      "Epoch 5 Batch 3300 Loss 0.6803\n",
      "Epoch 5 Batch 3400 Loss 1.0665\n",
      "Epoch 5 Batch 3500 Loss 3.4560\n",
      "Epoch 5 Batch 3600 Loss 0.0085\n",
      "Epoch 5 Batch 3700 Loss 1.0991\n",
      "Epoch 5 Batch 3800 Loss 0.9994\n",
      "Epoch 5 Batch 3900 Loss 2.0288\n",
      "Epoch 5 Batch 4000 Loss 1.4944\n",
      "Epoch 5 Batch 4100 Loss 1.5053\n",
      "Epoch 5 Batch 4200 Loss 0.4310\n",
      "Epoch 5 Batch 4300 Loss 2.0038\n",
      "Epoch 5 Batch 4400 Loss 1.9149\n",
      "Epoch 5 Batch 4500 Loss 0.5235\n",
      "Epoch 5 Batch 4600 Loss 3.3253\n",
      "Epoch 5 Batch 4700 Loss 2.4141\n",
      "Epoch 5 Batch 4800 Loss 1.2465\n",
      "Epoch 5 Batch 4900 Loss 0.4398\n",
      "Epoch 5 Batch 5000 Loss 2.3149\n",
      "Epoch 5 Batch 5100 Loss 0.1890\n",
      "Epoch 5 Batch 5200 Loss 0.9298\n",
      "Epoch 5 Batch 5300 Loss 2.2236\n",
      "Epoch 5 Batch 5400 Loss 0.4000\n",
      "Epoch 5 Batch 5500 Loss 0.8122\n",
      "Epoch 5 Batch 5600 Loss 2.0940\n",
      "Epoch 5 Batch 5700 Loss 1.6532\n",
      "Epoch 5 Batch 5800 Loss 1.9656\n",
      "Epoch 5 Batch 5900 Loss 0.1542\n",
      "Epoch 5 Batch 6000 Loss 2.0187\n",
      "Epoch 5 Batch 6100 Loss 3.2327\n",
      "Epoch 5 Batch 6200 Loss 1.7434\n",
      "Epoch 5 Batch 6300 Loss 1.1800\n",
      "Epoch 5 Batch 6400 Loss 0.5060\n",
      "Epoch 5 Batch 6500 Loss 2.7095\n",
      "Epoch 5 Batch 6600 Loss 0.6814\n",
      "Epoch 5 Batch 6700 Loss 2.9027\n",
      "Epoch 5 Batch 6800 Loss 1.9450\n",
      "Epoch 5 Batch 6900 Loss 3.0767\n",
      "Epoch 5 Batch 7000 Loss 0.1878\n",
      "Epoch 5 Batch 7100 Loss 0.1276\n",
      "Epoch 5 Batch 7200 Loss 1.3199\n",
      "Epoch 5 Batch 7300 Loss 0.0119\n",
      "Epoch 5 Batch 7400 Loss 2.6155\n",
      "Epoch 5 Batch 7500 Loss 0.9889\n",
      "Epoch 5 Batch 7600 Loss 1.6278\n",
      "Epoch 5 Batch 7700 Loss 0.3094\n",
      "Epoch 5 Batch 7800 Loss 1.1432\n",
      "Epoch 5 Batch 7900 Loss 3.2358\n",
      "Epoch 5 Batch 8000 Loss 1.2875\n",
      "Epoch 5 Batch 8100 Loss 5.0964\n",
      "Epoch 5 Batch 8200 Loss 0.7161\n",
      "Epoch 5 Batch 8300 Loss 1.9454\n",
      "Epoch 5 Batch 8400 Loss 2.2779\n",
      "Epoch 5 Batch 8500 Loss 0.1399\n",
      "Epoch 5 Batch 8600 Loss 2.1045\n",
      "Epoch 5 Batch 8700 Loss 1.7718\n",
      "Epoch 5 Batch 8800 Loss 1.4587\n",
      "Epoch 5 Batch 8900 Loss 3.0486\n",
      "Epoch 5 Batch 9000 Loss 0.0579\n",
      "Epoch 5 Batch 9100 Loss 3.8646\n",
      "Epoch 5 Batch 9200 Loss 3.9460\n",
      "Epoch 5 Batch 9300 Loss 1.7386\n",
      "Epoch 5 Batch 9400 Loss 1.2280\n",
      "Epoch 5 Batch 9500 Loss 0.7173\n",
      "Epoch 5 Batch 9600 Loss 3.6957\n",
      "Epoch 5 Batch 9700 Loss 0.5017\n",
      "Epoch 5 Batch 9800 Loss 3.9195\n",
      "Epoch 5 Batch 9900 Loss 0.7710\n",
      "Epoch 5 Batch 10000 Loss 2.1760\n",
      "Epoch 5 Batch 10100 Loss 0.7567\n",
      "Epoch 5 Batch 10200 Loss 0.0508\n",
      "Epoch 5 Batch 10300 Loss 3.7034\n",
      "Epoch 5 Batch 10400 Loss 0.6908\n",
      "Epoch 5 Batch 10500 Loss 2.1218\n",
      "Epoch 5 Batch 10600 Loss 2.6036\n",
      "Epoch 5 Batch 10700 Loss 1.4651\n",
      "Epoch 5 Batch 10800 Loss 1.3055\n",
      "Epoch 5 Batch 10900 Loss 2.4889\n",
      "Epoch 5 Batch 11000 Loss 1.9357\n",
      "Epoch 5 Batch 11100 Loss 0.0860\n",
      "Epoch 5 Batch 11200 Loss 1.0223\n",
      "Epoch 5 Batch 11300 Loss 4.6392\n",
      "Epoch 5 Batch 11400 Loss 3.4219\n",
      "Epoch 5 Batch 11500 Loss 0.5105\n",
      "Epoch 5 Batch 11600 Loss 2.5861\n",
      "Epoch 5 Batch 11700 Loss 2.0582\n",
      "Epoch 5 Batch 11800 Loss 3.2919\n",
      "Epoch 5 Batch 11900 Loss 0.2779\n",
      "Epoch 5 Batch 12000 Loss 3.6571\n",
      "Epoch 5 Batch 12100 Loss 0.0284\n",
      "Epoch 5 Batch 12200 Loss 2.7852\n",
      "Epoch 5 Batch 12300 Loss 3.4016\n",
      "Epoch 5 Batch 12400 Loss 0.9788\n",
      "Epoch 5 Batch 12500 Loss 3.8030\n",
      "Epoch 5 Batch 12600 Loss 1.9936\n",
      "Epoch 5 Batch 12700 Loss 4.6470\n",
      "Epoch 5 Batch 12800 Loss 3.2186\n",
      "Epoch 5 Batch 12900 Loss 4.5721\n",
      "Epoch 5 Batch 13000 Loss 0.0656\n",
      "Epoch 5 Batch 13100 Loss 0.6889\n",
      "Epoch 5 Batch 13200 Loss 2.2348\n",
      "Epoch 5 Batch 13300 Loss 2.5901\n",
      "Epoch 5 Batch 13400 Loss 2.7122\n",
      "Epoch 5 Batch 13500 Loss 0.4502\n",
      "Epoch 5 Batch 13600 Loss 1.9097\n",
      "Epoch 5 Batch 13700 Loss 2.8970\n",
      "Epoch 5 Batch 13800 Loss 4.3457\n",
      "Epoch 5 Batch 13900 Loss 1.6995\n",
      "Epoch 5 Batch 14000 Loss 0.4904\n",
      "Epoch 5 Batch 14100 Loss 0.4138\n",
      "Epoch 5 Batch 14200 Loss 0.4308\n",
      "Epoch 5 Batch 14300 Loss 1.5022\n",
      "Epoch 5 Batch 14400 Loss 1.2641\n",
      "Epoch 5 Batch 14500 Loss 2.5776\n",
      "Epoch 5 Batch 14600 Loss 1.0241\n",
      "Epoch 5 Batch 14700 Loss 3.1355\n",
      "Epoch 5 Batch 14800 Loss 2.0470\n",
      "Epoch 5 Batch 14900 Loss 1.5326\n",
      "Epoch 5 Batch 15000 Loss 3.6840\n",
      "Epoch 5 Batch 15100 Loss 2.5423\n",
      "Epoch 5 Batch 15200 Loss 0.9741\n",
      "Epoch 5 Batch 15300 Loss 4.7366\n",
      "Epoch 5 Batch 15400 Loss 4.7779\n",
      "Epoch 5 Batch 15500 Loss 2.7844\n",
      "Epoch 5 Batch 15600 Loss 0.0547\n",
      "Epoch 5 Batch 15700 Loss 0.0180\n",
      "Epoch 5 Batch 15800 Loss 2.1363\n",
      "Epoch 5 Batch 15900 Loss 3.5665\n",
      "Epoch 5 Batch 16000 Loss 0.0951\n",
      "Epoch 5 Batch 16100 Loss 1.6797\n",
      "Epoch 5 Batch 16200 Loss 0.5296\n",
      "Epoch 5 Batch 16300 Loss 2.3443\n",
      "Epoch 5 Batch 16400 Loss 0.0496\n",
      "Epoch 5 Batch 16500 Loss 3.7867\n",
      "Epoch 5 Batch 16600 Loss 1.0350\n",
      "Epoch 5 Batch 16700 Loss 0.3408\n",
      "Epoch 5 Batch 16800 Loss 0.8926\n",
      "Epoch 5 Batch 16900 Loss 3.7888\n",
      "Epoch 5 Batch 17000 Loss 2.1299\n",
      "Epoch 5 Batch 17100 Loss 3.4522\n",
      "Epoch 5 Loss 1.7845\n",
      "Time taken for 1 epoch 847.9353692531586 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 4.1083\n",
      "Epoch 6 Batch 100 Loss 2.7882\n",
      "Epoch 6 Batch 200 Loss 0.4073\n",
      "Epoch 6 Batch 300 Loss 3.9225\n",
      "Epoch 6 Batch 400 Loss 0.3388\n",
      "Epoch 6 Batch 500 Loss 0.1903\n",
      "Epoch 6 Batch 600 Loss 1.7906\n",
      "Epoch 6 Batch 700 Loss 2.5867\n",
      "Epoch 6 Batch 800 Loss 0.7109\n",
      "Epoch 6 Batch 900 Loss 0.8139\n",
      "Epoch 6 Batch 1000 Loss 0.0022\n",
      "Epoch 6 Batch 1100 Loss 0.9919\n",
      "Epoch 6 Batch 1200 Loss 2.0826\n",
      "Epoch 6 Batch 1300 Loss 0.0263\n",
      "Epoch 6 Batch 1400 Loss 1.7509\n",
      "Epoch 6 Batch 1500 Loss 0.1793\n",
      "Epoch 6 Batch 1600 Loss 2.1217\n",
      "Epoch 6 Batch 1700 Loss 2.4174\n",
      "Epoch 6 Batch 1800 Loss 3.2893\n",
      "Epoch 6 Batch 1900 Loss 0.0089\n",
      "Epoch 6 Batch 2000 Loss 0.3998\n",
      "Epoch 6 Batch 2100 Loss 1.7330\n",
      "Epoch 6 Batch 2200 Loss 0.2971\n",
      "Epoch 6 Batch 2300 Loss 2.8602\n",
      "Epoch 6 Batch 2400 Loss 0.7191\n",
      "Epoch 6 Batch 2500 Loss 1.1406\n",
      "Epoch 6 Batch 2600 Loss 1.1895\n",
      "Epoch 6 Batch 2700 Loss 0.6144\n",
      "Epoch 6 Batch 2800 Loss 1.6546\n",
      "Epoch 6 Batch 2900 Loss 1.4186\n",
      "Epoch 6 Batch 3000 Loss 1.1435\n",
      "Epoch 6 Batch 3100 Loss 2.1916\n",
      "Epoch 6 Batch 3200 Loss 1.2129\n",
      "Epoch 6 Batch 3300 Loss 1.5019\n",
      "Epoch 6 Batch 3400 Loss 1.4613\n",
      "Epoch 6 Batch 3500 Loss 1.3192\n",
      "Epoch 6 Batch 3600 Loss 0.1172\n",
      "Epoch 6 Batch 3700 Loss 1.7892\n",
      "Epoch 6 Batch 3800 Loss 2.2015\n",
      "Epoch 6 Batch 3900 Loss 1.0031\n",
      "Epoch 6 Batch 4000 Loss 2.0488\n",
      "Epoch 6 Batch 4100 Loss 0.5800\n",
      "Epoch 6 Batch 4200 Loss 0.4817\n",
      "Epoch 6 Batch 4300 Loss 0.8941\n",
      "Epoch 6 Batch 4400 Loss 1.7774\n",
      "Epoch 6 Batch 4500 Loss 2.3673\n",
      "Epoch 6 Batch 4600 Loss 1.9648\n",
      "Epoch 6 Batch 4700 Loss 0.8918\n",
      "Epoch 6 Batch 4800 Loss 0.4712\n",
      "Epoch 6 Batch 4900 Loss 3.5108\n",
      "Epoch 6 Batch 5000 Loss 0.0948\n",
      "Epoch 6 Batch 5100 Loss 0.0110\n",
      "Epoch 6 Batch 5200 Loss 0.2673\n",
      "Epoch 6 Batch 5300 Loss 0.9831\n",
      "Epoch 6 Batch 5400 Loss 0.4328\n",
      "Epoch 6 Batch 5500 Loss 1.1260\n",
      "Epoch 6 Batch 5600 Loss 1.9789\n",
      "Epoch 6 Batch 5700 Loss 0.2427\n",
      "Epoch 6 Batch 5800 Loss 0.0277\n",
      "Epoch 6 Batch 5900 Loss 3.2311\n",
      "Epoch 6 Batch 6000 Loss 0.4980\n",
      "Epoch 6 Batch 6100 Loss 2.5287\n",
      "Epoch 6 Batch 6200 Loss 4.3729\n",
      "Epoch 6 Batch 6300 Loss 2.0352\n",
      "Epoch 6 Batch 6400 Loss 0.2632\n",
      "Epoch 6 Batch 6500 Loss 0.2651\n",
      "Epoch 6 Batch 6600 Loss 2.2976\n",
      "Epoch 6 Batch 6700 Loss 0.0064\n",
      "Epoch 6 Batch 6800 Loss 1.7042\n",
      "Epoch 6 Batch 6900 Loss 0.9407\n",
      "Epoch 6 Batch 7000 Loss 0.3769\n",
      "Epoch 6 Batch 7100 Loss 2.6892\n",
      "Epoch 6 Batch 7200 Loss 0.3332\n",
      "Epoch 6 Batch 7300 Loss 1.2042\n",
      "Epoch 6 Batch 7400 Loss 0.0850\n",
      "Epoch 6 Batch 7500 Loss 0.0940\n",
      "Epoch 6 Batch 7600 Loss 0.9338\n",
      "Epoch 6 Batch 7700 Loss 2.8807\n",
      "Epoch 6 Batch 7800 Loss 3.4894\n",
      "Epoch 6 Batch 7900 Loss 3.7841\n",
      "Epoch 6 Batch 8000 Loss 0.2991\n",
      "Epoch 6 Batch 8100 Loss 3.1741\n",
      "Epoch 6 Batch 8200 Loss 0.0092\n",
      "Epoch 6 Batch 8300 Loss 0.2985\n",
      "Epoch 6 Batch 8400 Loss 1.2256\n",
      "Epoch 6 Batch 8500 Loss 3.0665\n",
      "Epoch 6 Batch 8600 Loss 0.0399\n",
      "Epoch 6 Batch 8700 Loss 3.0207\n",
      "Epoch 6 Batch 8800 Loss 3.8359\n",
      "Epoch 6 Batch 8900 Loss 2.0044\n",
      "Epoch 6 Batch 9000 Loss 0.0159\n",
      "Epoch 6 Batch 9100 Loss 2.2125\n",
      "Epoch 6 Batch 9200 Loss 3.5110\n",
      "Epoch 6 Batch 9300 Loss 3.5574\n",
      "Epoch 6 Batch 9400 Loss 1.2968\n",
      "Epoch 6 Batch 9500 Loss 1.7716\n",
      "Epoch 6 Batch 9600 Loss 4.0152\n",
      "Epoch 6 Batch 9700 Loss 0.4418\n",
      "Epoch 6 Batch 9800 Loss 0.1764\n",
      "Epoch 6 Batch 9900 Loss 2.4553\n",
      "Epoch 6 Batch 10000 Loss 2.4116\n",
      "Epoch 6 Batch 10100 Loss 0.9803\n",
      "Epoch 6 Batch 10200 Loss 2.9109\n",
      "Epoch 6 Batch 10300 Loss 0.6877\n",
      "Epoch 6 Batch 10400 Loss 2.7594\n",
      "Epoch 6 Batch 10500 Loss 3.0490\n",
      "Epoch 6 Batch 10600 Loss 0.3998\n",
      "Epoch 6 Batch 10700 Loss 3.0762\n",
      "Epoch 6 Batch 10800 Loss 2.9174\n",
      "Epoch 6 Batch 10900 Loss 1.6789\n",
      "Epoch 6 Batch 11000 Loss 3.7139\n",
      "Epoch 6 Batch 11100 Loss 0.7181\n",
      "Epoch 6 Batch 11200 Loss 1.8886\n",
      "Epoch 6 Batch 11300 Loss 3.9124\n",
      "Epoch 6 Batch 11400 Loss 2.9792\n",
      "Epoch 6 Batch 11500 Loss 2.6889\n",
      "Epoch 6 Batch 11600 Loss 1.4664\n",
      "Epoch 6 Batch 11700 Loss 1.4086\n",
      "Epoch 6 Batch 11800 Loss 0.1455\n",
      "Epoch 6 Batch 11900 Loss 1.9241\n",
      "Epoch 6 Batch 12000 Loss 1.4645\n",
      "Epoch 6 Batch 12100 Loss 1.5091\n",
      "Epoch 6 Batch 12200 Loss 0.6482\n",
      "Epoch 6 Batch 12300 Loss 1.6819\n",
      "Epoch 6 Batch 12400 Loss 0.8485\n",
      "Epoch 6 Batch 12500 Loss 2.7264\n",
      "Epoch 6 Batch 12600 Loss 2.1733\n",
      "Epoch 6 Batch 12700 Loss 4.3232\n",
      "Epoch 6 Batch 12800 Loss 1.6984\n",
      "Epoch 6 Batch 12900 Loss 3.1346\n",
      "Epoch 6 Batch 13000 Loss 1.7673\n",
      "Epoch 6 Batch 13100 Loss 0.5705\n",
      "Epoch 6 Batch 13200 Loss 2.3307\n",
      "Epoch 6 Batch 13300 Loss 2.9161\n",
      "Epoch 6 Batch 13400 Loss 1.0983\n",
      "Epoch 6 Batch 13500 Loss 0.0205\n",
      "Epoch 6 Batch 13600 Loss 3.5120\n",
      "Epoch 6 Batch 13700 Loss 2.1091\n",
      "Epoch 6 Batch 13800 Loss 1.3095\n",
      "Epoch 6 Batch 13900 Loss 3.3520\n",
      "Epoch 6 Batch 14000 Loss 3.2050\n",
      "Epoch 6 Batch 14100 Loss 3.0874\n",
      "Epoch 6 Batch 14200 Loss 1.3513\n",
      "Epoch 6 Batch 14300 Loss 2.1001\n",
      "Epoch 6 Batch 14400 Loss 1.5303\n",
      "Epoch 6 Batch 14500 Loss 0.3914\n",
      "Epoch 6 Batch 14600 Loss 0.2202\n",
      "Epoch 6 Batch 14700 Loss 3.8820\n",
      "Epoch 6 Batch 14800 Loss 0.2355\n",
      "Epoch 6 Batch 14900 Loss 0.6831\n",
      "Epoch 6 Batch 15000 Loss 2.1239\n",
      "Epoch 6 Batch 15100 Loss 0.0445\n",
      "Epoch 6 Batch 15200 Loss 5.5689\n",
      "Epoch 6 Batch 15300 Loss 0.5068\n",
      "Epoch 6 Batch 15400 Loss 1.6074\n",
      "Epoch 6 Batch 15500 Loss 0.0319\n",
      "Epoch 6 Batch 15600 Loss 1.7995\n",
      "Epoch 6 Batch 15700 Loss 0.0607\n",
      "Epoch 6 Batch 15800 Loss 2.8290\n",
      "Epoch 6 Batch 15900 Loss 1.1911\n",
      "Epoch 6 Batch 16000 Loss 4.1549\n",
      "Epoch 6 Batch 16100 Loss 3.8459\n",
      "Epoch 6 Batch 16200 Loss 4.1863\n",
      "Epoch 6 Batch 16300 Loss 2.6140\n",
      "Epoch 6 Batch 16400 Loss 3.2921\n",
      "Epoch 6 Batch 16500 Loss 0.3065\n",
      "Epoch 6 Batch 16600 Loss 3.9356\n",
      "Epoch 6 Batch 16700 Loss 0.0058\n",
      "Epoch 6 Batch 16800 Loss 2.6303\n",
      "Epoch 6 Batch 16900 Loss 1.6598\n",
      "Epoch 6 Batch 17000 Loss 1.3146\n",
      "Epoch 6 Batch 17100 Loss 4.6031\n",
      "Epoch 6 Loss 1.7748\n",
      "Time taken for 1 epoch 798.1870772838593 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.9377\n",
      "Epoch 7 Batch 100 Loss 2.9604\n",
      "Epoch 7 Batch 200 Loss 2.6686\n",
      "Epoch 7 Batch 300 Loss 0.0200\n",
      "Epoch 7 Batch 400 Loss 0.3264\n",
      "Epoch 7 Batch 500 Loss 0.5373\n",
      "Epoch 7 Batch 600 Loss 1.3861\n",
      "Epoch 7 Batch 700 Loss 2.6632\n",
      "Epoch 7 Batch 800 Loss 0.9660\n",
      "Epoch 7 Batch 900 Loss 2.0241\n",
      "Epoch 7 Batch 1000 Loss 1.0361\n",
      "Epoch 7 Batch 1100 Loss 0.0163\n",
      "Epoch 7 Batch 1200 Loss 1.4015\n",
      "Epoch 7 Batch 1300 Loss 0.0502\n",
      "Epoch 7 Batch 1400 Loss 0.0524\n",
      "Epoch 7 Batch 1500 Loss 0.0334\n",
      "Epoch 7 Batch 1600 Loss 0.0103\n",
      "Epoch 7 Batch 1700 Loss 2.1681\n",
      "Epoch 7 Batch 1800 Loss 1.6763\n",
      "Epoch 7 Batch 1900 Loss 2.0721\n",
      "Epoch 7 Batch 2000 Loss 2.1080\n",
      "Epoch 7 Batch 2100 Loss 3.1421\n",
      "Epoch 7 Batch 2200 Loss 0.4454\n",
      "Epoch 7 Batch 2300 Loss 1.2461\n",
      "Epoch 7 Batch 2400 Loss 1.0745\n",
      "Epoch 7 Batch 2500 Loss 1.8835\n",
      "Epoch 7 Batch 2600 Loss 0.8276\n",
      "Epoch 7 Batch 2700 Loss 2.5716\n",
      "Epoch 7 Batch 2800 Loss 3.4640\n",
      "Epoch 7 Batch 2900 Loss 2.7384\n",
      "Epoch 7 Batch 3000 Loss 1.7907\n",
      "Epoch 7 Batch 3100 Loss 0.0373\n",
      "Epoch 7 Batch 3200 Loss 0.0589\n",
      "Epoch 7 Batch 3300 Loss 0.0653\n",
      "Epoch 7 Batch 3400 Loss 0.1669\n",
      "Epoch 7 Batch 3500 Loss 2.1105\n",
      "Epoch 7 Batch 3600 Loss 1.3133\n",
      "Epoch 7 Batch 3700 Loss 1.3598\n",
      "Epoch 7 Batch 3800 Loss 1.6508\n",
      "Epoch 7 Batch 3900 Loss 1.3636\n",
      "Epoch 7 Batch 4000 Loss 2.4261\n",
      "Epoch 7 Batch 4100 Loss 2.9996\n",
      "Epoch 7 Batch 4200 Loss 1.0387\n",
      "Epoch 7 Batch 4300 Loss 3.4787\n",
      "Epoch 7 Batch 4400 Loss 2.4967\n",
      "Epoch 7 Batch 4500 Loss 0.6176\n",
      "Epoch 7 Batch 4600 Loss 0.9373\n",
      "Epoch 7 Batch 4700 Loss 2.4793\n",
      "Epoch 7 Batch 4800 Loss 0.0067\n",
      "Epoch 7 Batch 4900 Loss 2.9167\n",
      "Epoch 7 Batch 5000 Loss 1.5079\n",
      "Epoch 7 Batch 5100 Loss 0.0773\n",
      "Epoch 7 Batch 5200 Loss 1.4550\n",
      "Epoch 7 Batch 5300 Loss 3.2656\n",
      "Epoch 7 Batch 5400 Loss 2.2675\n",
      "Epoch 7 Batch 5500 Loss 0.2984\n",
      "Epoch 7 Batch 5600 Loss 1.7158\n",
      "Epoch 7 Batch 5700 Loss 1.7150\n",
      "Epoch 7 Batch 5800 Loss 2.5989\n",
      "Epoch 7 Batch 5900 Loss 0.9312\n",
      "Epoch 7 Batch 6000 Loss 2.2611\n",
      "Epoch 7 Batch 6100 Loss 2.6863\n",
      "Epoch 7 Batch 6200 Loss 2.8533\n",
      "Epoch 7 Batch 6300 Loss 1.1525\n",
      "Epoch 7 Batch 6400 Loss 1.6969\n",
      "Epoch 7 Batch 6500 Loss 1.4101\n",
      "Epoch 7 Batch 6600 Loss 1.8771\n",
      "Epoch 7 Batch 6700 Loss 2.9422\n",
      "Epoch 7 Batch 6800 Loss 0.2710\n",
      "Epoch 7 Batch 6900 Loss 3.0294\n",
      "Epoch 7 Batch 7000 Loss 2.0462\n",
      "Epoch 7 Batch 7100 Loss 2.3603\n",
      "Epoch 7 Batch 7200 Loss 2.8653\n",
      "Epoch 7 Batch 7300 Loss 1.1702\n",
      "Epoch 7 Batch 7400 Loss 1.0438\n",
      "Epoch 7 Batch 7500 Loss 1.2176\n",
      "Epoch 7 Batch 7600 Loss 1.7281\n",
      "Epoch 7 Batch 7700 Loss 2.8404\n",
      "Epoch 7 Batch 7800 Loss 0.5341\n",
      "Epoch 7 Batch 7900 Loss 1.8722\n",
      "Epoch 7 Batch 8000 Loss 0.9135\n",
      "Epoch 7 Batch 8100 Loss 1.7003\n",
      "Epoch 7 Batch 8200 Loss 0.7534\n",
      "Epoch 7 Batch 8300 Loss 1.2152\n",
      "Epoch 7 Batch 8400 Loss 1.9853\n",
      "Epoch 7 Batch 8500 Loss 0.2697\n",
      "Epoch 7 Batch 8600 Loss 0.3738\n",
      "Epoch 7 Batch 8700 Loss 0.6379\n",
      "Epoch 7 Batch 8800 Loss 3.5229\n",
      "Epoch 7 Batch 8900 Loss 1.2609\n",
      "Epoch 7 Batch 9000 Loss 0.3680\n",
      "Epoch 7 Batch 9100 Loss 0.2339\n",
      "Epoch 7 Batch 9200 Loss 4.6652\n",
      "Epoch 7 Batch 9300 Loss 0.7439\n",
      "Epoch 7 Batch 9400 Loss 0.5298\n",
      "Epoch 7 Batch 9500 Loss 2.2702\n",
      "Epoch 7 Batch 9600 Loss 3.8260\n",
      "Epoch 7 Batch 9700 Loss 2.3048\n",
      "Epoch 7 Batch 9800 Loss 3.4821\n",
      "Epoch 7 Batch 9900 Loss 0.6966\n",
      "Epoch 7 Batch 10000 Loss 2.5169\n",
      "Epoch 7 Batch 10100 Loss 2.3900\n",
      "Epoch 7 Batch 10200 Loss 2.9661\n",
      "Epoch 7 Batch 10300 Loss 1.0243\n",
      "Epoch 7 Batch 10400 Loss 2.0929\n",
      "Epoch 7 Batch 10500 Loss 3.2303\n",
      "Epoch 7 Batch 10600 Loss 3.9708\n",
      "Epoch 7 Batch 10700 Loss 0.8486\n",
      "Epoch 7 Batch 10800 Loss 3.2667\n",
      "Epoch 7 Batch 10900 Loss 0.2424\n",
      "Epoch 7 Batch 11000 Loss 2.4585\n",
      "Epoch 7 Batch 11100 Loss 3.0919\n",
      "Epoch 7 Batch 11200 Loss 2.0298\n",
      "Epoch 7 Batch 11300 Loss 3.1531\n",
      "Epoch 7 Batch 11400 Loss 0.2984\n",
      "Epoch 7 Batch 11500 Loss 4.1249\n",
      "Epoch 7 Batch 11600 Loss 0.4831\n",
      "Epoch 7 Batch 11700 Loss 2.6737\n",
      "Epoch 7 Batch 11800 Loss 0.0513\n",
      "Epoch 7 Batch 11900 Loss 0.0471\n",
      "Epoch 7 Batch 12000 Loss 0.7687\n",
      "Epoch 7 Batch 12100 Loss 1.3059\n",
      "Epoch 7 Batch 12200 Loss 3.4468\n",
      "Epoch 7 Batch 12300 Loss 2.8395\n",
      "Epoch 7 Batch 12400 Loss 0.9243\n",
      "Epoch 7 Batch 12500 Loss 0.2584\n",
      "Epoch 7 Batch 12600 Loss 0.6354\n",
      "Epoch 7 Batch 12700 Loss 4.7056\n",
      "Epoch 7 Batch 12800 Loss 2.0069\n",
      "Epoch 7 Batch 12900 Loss 1.2591\n",
      "Epoch 7 Batch 13000 Loss 1.6815\n",
      "Epoch 7 Batch 13100 Loss 1.8644\n",
      "Epoch 7 Batch 13200 Loss 2.7876\n",
      "Epoch 7 Batch 13300 Loss 1.5014\n",
      "Epoch 7 Batch 13400 Loss 2.8189\n",
      "Epoch 7 Batch 13500 Loss 0.9234\n",
      "Epoch 7 Batch 13600 Loss 0.9883\n",
      "Epoch 7 Batch 13700 Loss 2.5407\n",
      "Epoch 7 Batch 13800 Loss 0.2077\n",
      "Epoch 7 Batch 13900 Loss 2.7383\n",
      "Epoch 7 Batch 14000 Loss 2.6608\n",
      "Epoch 7 Batch 14100 Loss 3.1687\n",
      "Epoch 7 Batch 14200 Loss 2.3265\n",
      "Epoch 7 Batch 14300 Loss 2.2281\n",
      "Epoch 7 Batch 14400 Loss 0.3129\n",
      "Epoch 7 Batch 14500 Loss 4.8916\n",
      "Epoch 7 Batch 14600 Loss 0.1055\n",
      "Epoch 7 Batch 14700 Loss 0.7366\n",
      "Epoch 7 Batch 14800 Loss 0.6775\n",
      "Epoch 7 Batch 14900 Loss 2.5382\n",
      "Epoch 7 Batch 15000 Loss 0.7712\n",
      "Epoch 7 Batch 15100 Loss 4.5210\n",
      "Epoch 7 Batch 15200 Loss 0.1507\n",
      "Epoch 7 Batch 15300 Loss 0.0214\n",
      "Epoch 7 Batch 15400 Loss 2.1439\n",
      "Epoch 7 Batch 15500 Loss 4.2946\n",
      "Epoch 7 Batch 15600 Loss 4.9548\n",
      "Epoch 7 Batch 15700 Loss 0.0341\n",
      "Epoch 7 Batch 15800 Loss 0.3124\n",
      "Epoch 7 Batch 15900 Loss 0.3602\n",
      "Epoch 7 Batch 16000 Loss 0.3069\n",
      "Epoch 7 Batch 16100 Loss 3.5683\n",
      "Epoch 7 Batch 16200 Loss 1.2121\n",
      "Epoch 7 Batch 16300 Loss 2.0487\n",
      "Epoch 7 Batch 16400 Loss 0.9883\n",
      "Epoch 7 Batch 16500 Loss 3.7517\n",
      "Epoch 7 Batch 16600 Loss 2.2208\n",
      "Epoch 7 Batch 16700 Loss 0.7909\n",
      "Epoch 7 Batch 16800 Loss 2.7248\n",
      "Epoch 7 Batch 16900 Loss 1.9570\n",
      "Epoch 7 Batch 17000 Loss 2.9241\n",
      "Epoch 7 Batch 17100 Loss 0.0806\n",
      "Epoch 7 Loss 1.7482\n",
      "Time taken for 1 epoch 793.2322568893433 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.3185\n",
      "Epoch 8 Batch 100 Loss 2.1236\n",
      "Epoch 8 Batch 200 Loss 0.8565\n",
      "Epoch 8 Batch 300 Loss 3.3084\n",
      "Epoch 8 Batch 400 Loss 0.6981\n",
      "Epoch 8 Batch 500 Loss 1.6447\n",
      "Epoch 8 Batch 600 Loss 1.4804\n",
      "Epoch 8 Batch 700 Loss 0.8425\n",
      "Epoch 8 Batch 800 Loss 0.8378\n",
      "Epoch 8 Batch 900 Loss 1.3577\n",
      "Epoch 8 Batch 1000 Loss 1.0484\n",
      "Epoch 8 Batch 1100 Loss 0.4149\n",
      "Epoch 8 Batch 1200 Loss 3.6353\n",
      "Epoch 8 Batch 1300 Loss 0.9688\n",
      "Epoch 8 Batch 1400 Loss 2.2701\n",
      "Epoch 8 Batch 1500 Loss 0.3565\n",
      "Epoch 8 Batch 1600 Loss 3.5237\n",
      "Epoch 8 Batch 1700 Loss 1.2622\n",
      "Epoch 8 Batch 1800 Loss 3.1646\n",
      "Epoch 8 Batch 1900 Loss 1.6038\n",
      "Epoch 8 Batch 2000 Loss 2.0033\n",
      "Epoch 8 Batch 2100 Loss 1.1707\n",
      "Epoch 8 Batch 2200 Loss 1.4073\n",
      "Epoch 8 Batch 2300 Loss 1.4324\n",
      "Epoch 8 Batch 2400 Loss 3.7936\n",
      "Epoch 8 Batch 2500 Loss 0.3460\n",
      "Epoch 8 Batch 2600 Loss 1.5896\n",
      "Epoch 8 Batch 2700 Loss 0.6050\n",
      "Epoch 8 Batch 2800 Loss 3.1028\n",
      "Epoch 8 Batch 2900 Loss 2.9577\n",
      "Epoch 8 Batch 3000 Loss 0.0744\n",
      "Epoch 8 Batch 3100 Loss 0.5175\n",
      "Epoch 8 Batch 3200 Loss 1.9415\n",
      "Epoch 8 Batch 3300 Loss 1.8486\n",
      "Epoch 8 Batch 3400 Loss 0.7841\n",
      "Epoch 8 Batch 3500 Loss 1.2766\n",
      "Epoch 8 Batch 3600 Loss 1.5347\n",
      "Epoch 8 Batch 3700 Loss 1.1112\n",
      "Epoch 8 Batch 3800 Loss 2.3678\n",
      "Epoch 8 Batch 3900 Loss 0.1680\n",
      "Epoch 8 Batch 4000 Loss 0.2374\n",
      "Epoch 8 Batch 4100 Loss 0.1809\n",
      "Epoch 8 Batch 4200 Loss 0.7266\n",
      "Epoch 8 Batch 4300 Loss 2.4654\n",
      "Epoch 8 Batch 4400 Loss 2.1838\n",
      "Epoch 8 Batch 4500 Loss 1.3081\n",
      "Epoch 8 Batch 4600 Loss 2.7938\n",
      "Epoch 8 Batch 4700 Loss 3.2692\n",
      "Epoch 8 Batch 4800 Loss 0.8048\n",
      "Epoch 8 Batch 4900 Loss 1.4918\n",
      "Epoch 8 Batch 5000 Loss 0.6598\n",
      "Epoch 8 Batch 5100 Loss 0.0610\n",
      "Epoch 8 Batch 5200 Loss 3.1748\n",
      "Epoch 8 Batch 5300 Loss 2.0243\n",
      "Epoch 8 Batch 5400 Loss 0.2626\n",
      "Epoch 8 Batch 5500 Loss 0.1809\n",
      "Epoch 8 Batch 5600 Loss 1.4283\n",
      "Epoch 8 Batch 5700 Loss 0.7650\n",
      "Epoch 8 Batch 5800 Loss 2.4490\n",
      "Epoch 8 Batch 5900 Loss 2.0462\n",
      "Epoch 8 Batch 6000 Loss 1.2385\n",
      "Epoch 8 Batch 6100 Loss 2.1381\n",
      "Epoch 8 Batch 6200 Loss 0.9732\n",
      "Epoch 8 Batch 6300 Loss 2.8029\n",
      "Epoch 8 Batch 6400 Loss 2.5816\n",
      "Epoch 8 Batch 6500 Loss 0.4136\n",
      "Epoch 8 Batch 6600 Loss 3.3744\n",
      "Epoch 8 Batch 6700 Loss 0.8776\n",
      "Epoch 8 Batch 6800 Loss 3.1521\n",
      "Epoch 8 Batch 6900 Loss 2.6930\n",
      "Epoch 8 Batch 7000 Loss 1.1096\n",
      "Epoch 8 Batch 7100 Loss 3.0732\n",
      "Epoch 8 Batch 7200 Loss 0.5127\n",
      "Epoch 8 Batch 7300 Loss 0.5149\n",
      "Epoch 8 Batch 7400 Loss 4.1606\n",
      "Epoch 8 Batch 7500 Loss 1.1370\n",
      "Epoch 8 Batch 7600 Loss 0.0175\n",
      "Epoch 8 Batch 7700 Loss 3.5041\n",
      "Epoch 8 Batch 7800 Loss 0.0065\n",
      "Epoch 8 Batch 7900 Loss 1.2691\n",
      "Epoch 8 Batch 8000 Loss 1.9116\n",
      "Epoch 8 Batch 8100 Loss 1.8980\n",
      "Epoch 8 Batch 8200 Loss 0.9498\n",
      "Epoch 8 Batch 8300 Loss 1.1228\n",
      "Epoch 8 Batch 8400 Loss 0.1208\n",
      "Epoch 8 Batch 8500 Loss 1.9507\n",
      "Epoch 8 Batch 8600 Loss 0.0272\n",
      "Epoch 8 Batch 8700 Loss 1.4962\n",
      "Epoch 8 Batch 8800 Loss 0.9480\n",
      "Epoch 8 Batch 8900 Loss 0.0647\n",
      "Epoch 8 Batch 9000 Loss 0.5575\n",
      "Epoch 8 Batch 9100 Loss 0.0545\n",
      "Epoch 8 Batch 9200 Loss 3.7141\n",
      "Epoch 8 Batch 9300 Loss 2.9105\n",
      "Epoch 8 Batch 9400 Loss 1.4732\n",
      "Epoch 8 Batch 9500 Loss 1.5462\n",
      "Epoch 8 Batch 9600 Loss 3.6009\n",
      "Epoch 8 Batch 9700 Loss 0.6328\n",
      "Epoch 8 Batch 9800 Loss 1.9060\n",
      "Epoch 8 Batch 9900 Loss 3.0430\n",
      "Epoch 8 Batch 10000 Loss 2.3788\n",
      "Epoch 8 Batch 10100 Loss 3.4241\n",
      "Epoch 8 Batch 10200 Loss 2.0055\n",
      "Epoch 8 Batch 10300 Loss 3.7025\n",
      "Epoch 8 Batch 10400 Loss 3.8214\n",
      "Epoch 8 Batch 10500 Loss 1.0033\n",
      "Epoch 8 Batch 10600 Loss 0.9101\n",
      "Epoch 8 Batch 10700 Loss 3.2860\n",
      "Epoch 8 Batch 10800 Loss 0.0893\n",
      "Epoch 8 Batch 10900 Loss 2.3587\n",
      "Epoch 8 Batch 11000 Loss 0.5620\n",
      "Epoch 8 Batch 11100 Loss 3.1643\n",
      "Epoch 8 Batch 11200 Loss 1.7581\n",
      "Epoch 8 Batch 11300 Loss 2.0545\n",
      "Epoch 8 Batch 11400 Loss 4.4606\n",
      "Epoch 8 Batch 11500 Loss 3.5464\n",
      "Epoch 8 Batch 11600 Loss 2.7606\n",
      "Epoch 8 Batch 11700 Loss 1.9794\n",
      "Epoch 8 Batch 11800 Loss 1.3170\n",
      "Epoch 8 Batch 11900 Loss 0.8588\n",
      "Epoch 8 Batch 12000 Loss 4.2829\n",
      "Epoch 8 Batch 12100 Loss 2.7428\n",
      "Epoch 8 Batch 12200 Loss 2.9072\n",
      "Epoch 8 Batch 12300 Loss 1.0648\n",
      "Epoch 8 Batch 12400 Loss 0.3252\n",
      "Epoch 8 Batch 12500 Loss 2.2670\n",
      "Epoch 8 Batch 12600 Loss 0.5657\n",
      "Epoch 8 Batch 12700 Loss 0.5577\n",
      "Epoch 8 Batch 12800 Loss 4.8201\n",
      "Epoch 8 Batch 12900 Loss 2.0117\n",
      "Epoch 8 Batch 13000 Loss 1.4634\n",
      "Epoch 8 Batch 13100 Loss 0.9140\n",
      "Epoch 8 Batch 13200 Loss 4.0674\n",
      "Epoch 8 Batch 13300 Loss 4.0820\n",
      "Epoch 8 Batch 13400 Loss 3.4408\n",
      "Epoch 8 Batch 13500 Loss 1.6348\n",
      "Epoch 8 Batch 13600 Loss 0.2228\n",
      "Epoch 8 Batch 13700 Loss 4.7487\n",
      "Epoch 8 Batch 13800 Loss 0.3358\n",
      "Epoch 8 Batch 13900 Loss 3.2584\n",
      "Epoch 8 Batch 14000 Loss 0.2692\n",
      "Epoch 8 Batch 14100 Loss 1.5301\n",
      "Epoch 8 Batch 14200 Loss 1.6876\n",
      "Epoch 8 Batch 14300 Loss 2.0178\n",
      "Epoch 8 Batch 14400 Loss 0.6902\n",
      "Epoch 8 Batch 14500 Loss 0.5660\n",
      "Epoch 8 Batch 14600 Loss 0.0361\n",
      "Epoch 8 Batch 14700 Loss 3.5664\n",
      "Epoch 8 Batch 14800 Loss 1.3972\n",
      "Epoch 8 Batch 14900 Loss 3.5271\n",
      "Epoch 8 Batch 15000 Loss 2.5315\n",
      "Epoch 8 Batch 15100 Loss 0.0309\n",
      "Epoch 8 Batch 15200 Loss 0.6626\n",
      "Epoch 8 Batch 15300 Loss 4.6342\n",
      "Epoch 8 Batch 15400 Loss 1.1957\n",
      "Epoch 8 Batch 15500 Loss 3.6187\n",
      "Epoch 8 Batch 15600 Loss 4.5743\n",
      "Epoch 8 Batch 15700 Loss 0.6193\n",
      "Epoch 8 Batch 15800 Loss 3.4432\n",
      "Epoch 8 Batch 15900 Loss 3.3628\n",
      "Epoch 8 Batch 16000 Loss 0.7281\n",
      "Epoch 8 Batch 16100 Loss 1.5795\n",
      "Epoch 8 Batch 16200 Loss 0.3008\n",
      "Epoch 8 Batch 16300 Loss 0.2580\n",
      "Epoch 8 Batch 16400 Loss 3.1638\n",
      "Epoch 8 Batch 16500 Loss 0.5946\n",
      "Epoch 8 Batch 16600 Loss 1.2032\n",
      "Epoch 8 Batch 16700 Loss 3.7270\n",
      "Epoch 8 Batch 16800 Loss 4.3675\n",
      "Epoch 8 Batch 16900 Loss 3.7926\n",
      "Epoch 8 Batch 17000 Loss 3.5376\n",
      "Epoch 8 Batch 17100 Loss 1.1468\n",
      "Epoch 8 Loss 1.7455\n",
      "Time taken for 1 epoch 759.4054598808289 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.7128\n",
      "Epoch 9 Batch 100 Loss 1.6483\n",
      "Epoch 9 Batch 200 Loss 1.1646\n",
      "Epoch 9 Batch 300 Loss 1.0803\n",
      "Epoch 9 Batch 400 Loss 1.2392\n",
      "Epoch 9 Batch 500 Loss 0.8837\n",
      "Epoch 9 Batch 600 Loss 0.0602\n",
      "Epoch 9 Batch 700 Loss 1.4453\n",
      "Epoch 9 Batch 800 Loss 0.4768\n",
      "Epoch 9 Batch 900 Loss 3.4182\n",
      "Epoch 9 Batch 1000 Loss 0.0462\n",
      "Epoch 9 Batch 1100 Loss 2.3530\n",
      "Epoch 9 Batch 1200 Loss 1.9939\n",
      "Epoch 9 Batch 1300 Loss 0.2166\n",
      "Epoch 9 Batch 1400 Loss 0.8438\n",
      "Epoch 9 Batch 1500 Loss 3.6235\n",
      "Epoch 9 Batch 1600 Loss 1.9861\n",
      "Epoch 9 Batch 1700 Loss 0.5024\n",
      "Epoch 9 Batch 1800 Loss 2.7148\n",
      "Epoch 9 Batch 1900 Loss 0.1363\n",
      "Epoch 9 Batch 2000 Loss 0.6563\n",
      "Epoch 9 Batch 2100 Loss 1.4916\n",
      "Epoch 9 Batch 2200 Loss 1.8206\n",
      "Epoch 9 Batch 2300 Loss 3.1645\n",
      "Epoch 9 Batch 2400 Loss 2.7953\n",
      "Epoch 9 Batch 2500 Loss 2.6477\n",
      "Epoch 9 Batch 2600 Loss 2.0056\n",
      "Epoch 9 Batch 2700 Loss 2.8284\n",
      "Epoch 9 Batch 2800 Loss 0.1600\n",
      "Epoch 9 Batch 2900 Loss 0.9534\n",
      "Epoch 9 Batch 3000 Loss 1.6264\n",
      "Epoch 9 Batch 3100 Loss 3.2798\n",
      "Epoch 9 Batch 3200 Loss 1.0842\n",
      "Epoch 9 Batch 3300 Loss 1.9311\n",
      "Epoch 9 Batch 3400 Loss 2.1570\n",
      "Epoch 9 Batch 3500 Loss 1.5236\n",
      "Epoch 9 Batch 3600 Loss 1.5993\n",
      "Epoch 9 Batch 3700 Loss 2.6079\n",
      "Epoch 9 Batch 3800 Loss 1.7690\n",
      "Epoch 9 Batch 3900 Loss 1.9584\n",
      "Epoch 9 Batch 4000 Loss 0.2680\n",
      "Epoch 9 Batch 4100 Loss 1.2517\n",
      "Epoch 9 Batch 4200 Loss 1.1420\n",
      "Epoch 9 Batch 4300 Loss 1.6437\n",
      "Epoch 9 Batch 4400 Loss 2.3546\n",
      "Epoch 9 Batch 4500 Loss 0.1953\n",
      "Epoch 9 Batch 4600 Loss 1.1493\n",
      "Epoch 9 Batch 4700 Loss 2.7394\n",
      "Epoch 9 Batch 4800 Loss 0.5950\n",
      "Epoch 9 Batch 4900 Loss 3.3097\n",
      "Epoch 9 Batch 5000 Loss 0.1096\n",
      "Epoch 9 Batch 5100 Loss 2.9448\n",
      "Epoch 9 Batch 5200 Loss 3.4951\n",
      "Epoch 9 Batch 5300 Loss 0.5693\n",
      "Epoch 9 Batch 5400 Loss 3.2208\n",
      "Epoch 9 Batch 5500 Loss 1.3270\n",
      "Epoch 9 Batch 5600 Loss 1.1887\n",
      "Epoch 9 Batch 5700 Loss 3.2117\n",
      "Epoch 9 Batch 5800 Loss 0.6832\n",
      "Epoch 9 Batch 5900 Loss 2.7663\n",
      "Epoch 9 Batch 6000 Loss 0.0246\n",
      "Epoch 9 Batch 6100 Loss 1.8941\n",
      "Epoch 9 Batch 6200 Loss 0.9530\n",
      "Epoch 9 Batch 6300 Loss 2.8294\n",
      "Epoch 9 Batch 6400 Loss 3.3745\n",
      "Epoch 9 Batch 6500 Loss 2.1964\n",
      "Epoch 9 Batch 6600 Loss 0.4593\n",
      "Epoch 9 Batch 6700 Loss 1.7651\n",
      "Epoch 9 Batch 6800 Loss 1.6645\n",
      "Epoch 9 Batch 6900 Loss 0.7278\n",
      "Epoch 9 Batch 7000 Loss 3.1059\n",
      "Epoch 9 Batch 7100 Loss 3.1158\n",
      "Epoch 9 Batch 7200 Loss 0.2265\n",
      "Epoch 9 Batch 7300 Loss 0.0132\n",
      "Epoch 9 Batch 7400 Loss 2.4628\n",
      "Epoch 9 Batch 7500 Loss 1.9359\n",
      "Epoch 9 Batch 7600 Loss 2.9413\n",
      "Epoch 9 Batch 7700 Loss 3.2056\n",
      "Epoch 9 Batch 7800 Loss 1.5302\n",
      "Epoch 9 Batch 7900 Loss 4.1797\n",
      "Epoch 9 Batch 8000 Loss 0.9734\n",
      "Epoch 9 Batch 8100 Loss 3.3323\n",
      "Epoch 9 Batch 8200 Loss 0.8543\n",
      "Epoch 9 Batch 8300 Loss 0.7707\n",
      "Epoch 9 Batch 8400 Loss 1.3755\n",
      "Epoch 9 Batch 8500 Loss 1.6328\n",
      "Epoch 9 Batch 8600 Loss 1.1055\n",
      "Epoch 9 Batch 8700 Loss 2.0107\n",
      "Epoch 9 Batch 8800 Loss 4.1303\n",
      "Epoch 9 Batch 8900 Loss 2.1024\n",
      "Epoch 9 Batch 9000 Loss 0.4460\n",
      "Epoch 9 Batch 9100 Loss 0.9093\n",
      "Epoch 9 Batch 9200 Loss 0.2049\n",
      "Epoch 9 Batch 9300 Loss 1.0480\n",
      "Epoch 9 Batch 9400 Loss 1.9184\n",
      "Epoch 9 Batch 9500 Loss 2.3385\n",
      "Epoch 9 Batch 9600 Loss 2.7631\n",
      "Epoch 9 Batch 9700 Loss 0.1654\n",
      "Epoch 9 Batch 9800 Loss 1.8693\n",
      "Epoch 9 Batch 9900 Loss 0.1500\n",
      "Epoch 9 Batch 10000 Loss 1.5019\n",
      "Epoch 9 Batch 10100 Loss 0.0744\n",
      "Epoch 9 Batch 10200 Loss 0.3193\n",
      "Epoch 9 Batch 10300 Loss 2.5924\n",
      "Epoch 9 Batch 10400 Loss 3.6358\n",
      "Epoch 9 Batch 10500 Loss 0.8914\n",
      "Epoch 9 Batch 10600 Loss 1.4210\n",
      "Epoch 9 Batch 10700 Loss 3.1121\n",
      "Epoch 9 Batch 10800 Loss 2.2594\n",
      "Epoch 9 Batch 10900 Loss 1.8919\n",
      "Epoch 9 Batch 11000 Loss 0.0456\n",
      "Epoch 9 Batch 11100 Loss 3.2714\n",
      "Epoch 9 Batch 11200 Loss 1.3209\n",
      "Epoch 9 Batch 11300 Loss 0.4012\n",
      "Epoch 9 Batch 11400 Loss 1.9653\n",
      "Epoch 9 Batch 11500 Loss 0.1099\n",
      "Epoch 9 Batch 11600 Loss 1.4549\n",
      "Epoch 9 Batch 11700 Loss 0.6063\n",
      "Epoch 9 Batch 11800 Loss 1.1228\n",
      "Epoch 9 Batch 11900 Loss 2.4078\n",
      "Epoch 9 Batch 12000 Loss 3.2928\n",
      "Epoch 9 Batch 12100 Loss 2.5823\n",
      "Epoch 9 Batch 12200 Loss 2.1314\n",
      "Epoch 9 Batch 12300 Loss 2.8313\n",
      "Epoch 9 Batch 12400 Loss 0.3585\n",
      "Epoch 9 Batch 12500 Loss 1.1663\n",
      "Epoch 9 Batch 12600 Loss 2.7679\n",
      "Epoch 9 Batch 12700 Loss 0.4846\n",
      "Epoch 9 Batch 12800 Loss 3.1640\n",
      "Epoch 9 Batch 12900 Loss 2.5951\n",
      "Epoch 9 Batch 13000 Loss 2.2023\n",
      "Epoch 9 Batch 13100 Loss 1.4938\n",
      "Epoch 9 Batch 13200 Loss 0.0436\n",
      "Epoch 9 Batch 13300 Loss 0.4790\n",
      "Epoch 9 Batch 13400 Loss 2.7803\n",
      "Epoch 9 Batch 13500 Loss 1.6612\n",
      "Epoch 9 Batch 13600 Loss 1.4515\n",
      "Epoch 9 Batch 13700 Loss 3.3804\n",
      "Epoch 9 Batch 13800 Loss 0.0060\n",
      "Epoch 9 Batch 13900 Loss 1.2617\n",
      "Epoch 9 Batch 14000 Loss 0.4130\n",
      "Epoch 9 Batch 14100 Loss 3.8293\n",
      "Epoch 9 Batch 14200 Loss 1.2142\n",
      "Epoch 9 Batch 14300 Loss 0.0400\n",
      "Epoch 9 Batch 14400 Loss 0.3107\n",
      "Epoch 9 Batch 14500 Loss 0.6833\n",
      "Epoch 9 Batch 14600 Loss 0.3035\n",
      "Epoch 9 Batch 14700 Loss 4.6054\n",
      "Epoch 9 Batch 14800 Loss 0.4338\n",
      "Epoch 9 Batch 14900 Loss 2.4263\n",
      "Epoch 9 Batch 15000 Loss 3.5838\n",
      "Epoch 9 Batch 15100 Loss 1.5503\n",
      "Epoch 9 Batch 15200 Loss 0.4107\n",
      "Epoch 9 Batch 15300 Loss 1.1551\n",
      "Epoch 9 Batch 15400 Loss 3.3470\n",
      "Epoch 9 Batch 15500 Loss 2.0381\n",
      "Epoch 9 Batch 15600 Loss 3.4341\n",
      "Epoch 9 Batch 15700 Loss 0.5896\n",
      "Epoch 9 Batch 15800 Loss 5.9678\n",
      "Epoch 9 Batch 15900 Loss 3.0887\n",
      "Epoch 9 Batch 16000 Loss 1.6425\n",
      "Epoch 9 Batch 16100 Loss 1.1333\n",
      "Epoch 9 Batch 16200 Loss 3.1680\n",
      "Epoch 9 Batch 16300 Loss 2.5662\n",
      "Epoch 9 Batch 16400 Loss 2.9240\n",
      "Epoch 9 Batch 16500 Loss 1.0087\n",
      "Epoch 9 Batch 16600 Loss 0.0934\n",
      "Epoch 9 Batch 16700 Loss 1.1890\n",
      "Epoch 9 Batch 16800 Loss 2.9559\n",
      "Epoch 9 Batch 16900 Loss 2.8139\n",
      "Epoch 9 Batch 17000 Loss 0.6515\n",
      "Epoch 9 Batch 17100 Loss 0.4020\n",
      "Epoch 9 Loss 1.7400\n",
      "Time taken for 1 epoch 774.7532148361206 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0180\n",
      "Epoch 10 Batch 100 Loss 0.7427\n",
      "Epoch 10 Batch 200 Loss 0.1981\n",
      "Epoch 10 Batch 300 Loss 0.6401\n",
      "Epoch 10 Batch 400 Loss 0.8905\n",
      "Epoch 10 Batch 500 Loss 2.3341\n",
      "Epoch 10 Batch 600 Loss 0.5775\n",
      "Epoch 10 Batch 700 Loss 1.3900\n",
      "Epoch 10 Batch 800 Loss 1.7532\n",
      "Epoch 10 Batch 900 Loss 0.9629\n",
      "Epoch 10 Batch 1000 Loss 1.9183\n",
      "Epoch 10 Batch 1100 Loss 2.1234\n",
      "Epoch 10 Batch 1200 Loss 2.7239\n",
      "Epoch 10 Batch 1300 Loss 3.4729\n",
      "Epoch 10 Batch 1400 Loss 2.5438\n",
      "Epoch 10 Batch 1500 Loss 0.7153\n",
      "Epoch 10 Batch 1600 Loss 1.8664\n",
      "Epoch 10 Batch 1700 Loss 0.1124\n",
      "Epoch 10 Batch 1800 Loss 1.5443\n",
      "Epoch 10 Batch 1900 Loss 3.0180\n",
      "Epoch 10 Batch 2000 Loss 0.7007\n",
      "Epoch 10 Batch 2100 Loss 0.6963\n",
      "Epoch 10 Batch 2200 Loss 1.8605\n",
      "Epoch 10 Batch 2300 Loss 2.1890\n",
      "Epoch 10 Batch 2400 Loss 2.0015\n",
      "Epoch 10 Batch 2500 Loss 2.5106\n",
      "Epoch 10 Batch 2600 Loss 0.4288\n",
      "Epoch 10 Batch 2700 Loss 2.9562\n",
      "Epoch 10 Batch 2800 Loss 1.8231\n",
      "Epoch 10 Batch 2900 Loss 0.0670\n",
      "Epoch 10 Batch 3000 Loss 0.4826\n",
      "Epoch 10 Batch 3100 Loss 2.5231\n",
      "Epoch 10 Batch 3200 Loss 2.0201\n",
      "Epoch 10 Batch 3300 Loss 0.5801\n",
      "Epoch 10 Batch 3400 Loss 0.9276\n",
      "Epoch 10 Batch 3500 Loss 1.5539\n",
      "Epoch 10 Batch 3600 Loss 2.1060\n",
      "Epoch 10 Batch 3700 Loss 2.4164\n",
      "Epoch 10 Batch 3800 Loss 0.1742\n",
      "Epoch 10 Batch 3900 Loss 3.0203\n",
      "Epoch 10 Batch 4000 Loss 0.3510\n",
      "Epoch 10 Batch 4100 Loss 1.3065\n",
      "Epoch 10 Batch 4200 Loss 1.4502\n",
      "Epoch 10 Batch 4300 Loss 1.5768\n",
      "Epoch 10 Batch 4400 Loss 1.2507\n",
      "Epoch 10 Batch 4500 Loss 0.4706\n",
      "Epoch 10 Batch 4600 Loss 0.6629\n",
      "Epoch 10 Batch 4700 Loss 1.0901\n",
      "Epoch 10 Batch 4800 Loss 2.6253\n",
      "Epoch 10 Batch 4900 Loss 2.0142\n",
      "Epoch 10 Batch 5000 Loss 2.2329\n",
      "Epoch 10 Batch 5100 Loss 0.8390\n",
      "Epoch 10 Batch 5200 Loss 1.7326\n",
      "Epoch 10 Batch 5300 Loss 2.2299\n",
      "Epoch 10 Batch 5400 Loss 0.0778\n",
      "Epoch 10 Batch 5500 Loss 1.7325\n",
      "Epoch 10 Batch 5600 Loss 2.5515\n",
      "Epoch 10 Batch 5700 Loss 0.1259\n",
      "Epoch 10 Batch 5800 Loss 1.6117\n",
      "Epoch 10 Batch 5900 Loss 2.7798\n",
      "Epoch 10 Batch 6000 Loss 3.8409\n",
      "Epoch 10 Batch 6100 Loss 0.9647\n",
      "Epoch 10 Batch 6200 Loss 1.7419\n",
      "Epoch 10 Batch 6300 Loss 1.2049\n",
      "Epoch 10 Batch 6400 Loss 0.9419\n",
      "Epoch 10 Batch 6500 Loss 0.2091\n",
      "Epoch 10 Batch 6600 Loss 0.1380\n",
      "Epoch 10 Batch 6700 Loss 1.0590\n",
      "Epoch 10 Batch 6800 Loss 0.1114\n",
      "Epoch 10 Batch 6900 Loss 3.7680\n",
      "Epoch 10 Batch 7000 Loss 0.0220\n",
      "Epoch 10 Batch 7100 Loss 4.8794\n",
      "Epoch 10 Batch 7200 Loss 0.8233\n",
      "Epoch 10 Batch 7300 Loss 1.5214\n",
      "Epoch 10 Batch 7400 Loss 4.1908\n",
      "Epoch 10 Batch 7500 Loss 3.3520\n",
      "Epoch 10 Batch 7600 Loss 1.6005\n",
      "Epoch 10 Batch 7700 Loss 2.8216\n",
      "Epoch 10 Batch 7800 Loss 0.1040\n",
      "Epoch 10 Batch 7900 Loss 1.0935\n",
      "Epoch 10 Batch 8000 Loss 0.8847\n",
      "Epoch 10 Batch 8100 Loss 2.7646\n",
      "Epoch 10 Batch 8200 Loss 3.0247\n",
      "Epoch 10 Batch 8300 Loss 2.8445\n",
      "Epoch 10 Batch 8400 Loss 1.6526\n",
      "Epoch 10 Batch 8500 Loss 2.5631\n",
      "Epoch 10 Batch 8600 Loss 1.8759\n",
      "Epoch 10 Batch 8700 Loss 0.6121\n",
      "Epoch 10 Batch 8800 Loss 1.0423\n",
      "Epoch 10 Batch 8900 Loss 0.3275\n",
      "Epoch 10 Batch 9000 Loss 0.0595\n",
      "Epoch 10 Batch 9100 Loss 3.4749\n",
      "Epoch 10 Batch 9200 Loss 1.2176\n",
      "Epoch 10 Batch 9300 Loss 1.8080\n",
      "Epoch 10 Batch 9400 Loss 1.0164\n",
      "Epoch 10 Batch 9500 Loss 0.9514\n",
      "Epoch 10 Batch 9600 Loss 2.8528\n",
      "Epoch 10 Batch 9700 Loss 2.1627\n",
      "Epoch 10 Batch 9800 Loss 0.9344\n",
      "Epoch 10 Batch 9900 Loss 1.6820\n",
      "Epoch 10 Batch 10000 Loss 1.2085\n",
      "Epoch 10 Batch 10100 Loss 2.3702\n",
      "Epoch 10 Batch 10200 Loss 0.4886\n",
      "Epoch 10 Batch 10300 Loss 0.4136\n",
      "Epoch 10 Batch 10400 Loss 6.6982\n",
      "Epoch 10 Batch 10500 Loss 1.2183\n",
      "Epoch 10 Batch 10600 Loss 1.5165\n",
      "Epoch 10 Batch 10700 Loss 0.1167\n",
      "Epoch 10 Batch 10800 Loss 3.3774\n",
      "Epoch 10 Batch 10900 Loss 3.9498\n",
      "Epoch 10 Batch 11000 Loss 2.6940\n",
      "Epoch 10 Batch 11100 Loss 2.7816\n",
      "Epoch 10 Batch 11200 Loss 0.0528\n",
      "Epoch 10 Batch 11300 Loss 2.6015\n",
      "Epoch 10 Batch 11400 Loss 1.0853\n",
      "Epoch 10 Batch 11500 Loss 2.2102\n",
      "Epoch 10 Batch 11600 Loss 2.2622\n",
      "Epoch 10 Batch 11700 Loss 0.4528\n",
      "Epoch 10 Batch 11800 Loss 3.6968\n",
      "Epoch 10 Batch 11900 Loss 0.8863\n",
      "Epoch 10 Batch 12000 Loss 1.6540\n",
      "Epoch 10 Batch 12100 Loss 1.8847\n",
      "Epoch 10 Batch 12200 Loss 2.8878\n",
      "Epoch 10 Batch 12300 Loss 3.2418\n",
      "Epoch 10 Batch 12400 Loss 3.4685\n",
      "Epoch 10 Batch 12500 Loss 3.4070\n",
      "Epoch 10 Batch 12600 Loss 0.6710\n",
      "Epoch 10 Batch 12700 Loss 0.0979\n",
      "Epoch 10 Batch 12800 Loss 4.5935\n",
      "Epoch 10 Batch 12900 Loss 2.1788\n",
      "Epoch 10 Batch 13000 Loss 2.8801\n",
      "Epoch 10 Batch 13100 Loss 4.2361\n",
      "Epoch 10 Batch 13200 Loss 2.5172\n",
      "Epoch 10 Batch 13300 Loss 2.0376\n",
      "Epoch 10 Batch 13400 Loss 1.7914\n",
      "Epoch 10 Batch 13500 Loss 1.9929\n",
      "Epoch 10 Batch 13600 Loss 1.3381\n",
      "Epoch 10 Batch 13700 Loss 1.3519\n",
      "Epoch 10 Batch 13800 Loss 3.2060\n",
      "Epoch 10 Batch 13900 Loss 3.2924\n",
      "Epoch 10 Batch 14000 Loss 1.0386\n",
      "Epoch 10 Batch 14100 Loss 0.6114\n",
      "Epoch 10 Batch 14200 Loss 2.7998\n",
      "Epoch 10 Batch 14300 Loss 1.6596\n",
      "Epoch 10 Batch 14400 Loss 4.5526\n",
      "Epoch 10 Batch 14500 Loss 3.2771\n",
      "Epoch 10 Batch 14600 Loss 3.0181\n",
      "Epoch 10 Batch 14700 Loss 2.8828\n",
      "Epoch 10 Batch 14800 Loss 1.0599\n",
      "Epoch 10 Batch 14900 Loss 0.0304\n",
      "Epoch 10 Batch 15000 Loss 1.0814\n",
      "Epoch 10 Batch 15100 Loss 2.4707\n",
      "Epoch 10 Batch 15200 Loss 5.0983\n",
      "Epoch 10 Batch 15300 Loss 1.9574\n",
      "Epoch 10 Batch 15400 Loss 1.4548\n",
      "Epoch 10 Batch 15500 Loss 0.7016\n",
      "Epoch 10 Batch 15600 Loss 4.5926\n",
      "Epoch 10 Batch 15700 Loss 1.2660\n",
      "Epoch 10 Batch 15800 Loss 2.3085\n",
      "Epoch 10 Batch 15900 Loss 2.0557\n",
      "Epoch 10 Batch 16000 Loss 2.6802\n",
      "Epoch 10 Batch 16100 Loss 3.1447\n",
      "Epoch 10 Batch 16200 Loss 3.1661\n",
      "Epoch 10 Batch 16300 Loss 1.1048\n",
      "Epoch 10 Batch 16400 Loss 0.4294\n",
      "Epoch 10 Batch 16500 Loss 0.9717\n",
      "Epoch 10 Batch 16600 Loss 0.2487\n",
      "Epoch 10 Batch 16700 Loss 0.1931\n",
      "Epoch 10 Batch 16800 Loss 0.0973\n",
      "Epoch 10 Batch 16900 Loss 2.6896\n",
      "Epoch 10 Batch 17000 Loss 4.4328\n",
      "Epoch 10 Batch 17100 Loss 1.4578\n",
      "Epoch 10 Loss 1.7437\n",
      "Time taken for 1 epoch 1049.41219997406 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 3.2880\n",
      "Epoch 11 Batch 100 Loss 2.5460\n",
      "Epoch 11 Batch 200 Loss 1.3058\n",
      "Epoch 11 Batch 300 Loss 0.1756\n",
      "Epoch 11 Batch 400 Loss 2.0560\n",
      "Epoch 11 Batch 500 Loss 2.4579\n",
      "Epoch 11 Batch 600 Loss 0.5015\n",
      "Epoch 11 Batch 700 Loss 1.8715\n",
      "Epoch 11 Batch 800 Loss 1.0331\n",
      "Epoch 11 Batch 900 Loss 0.7073\n",
      "Epoch 11 Batch 1000 Loss 0.8867\n",
      "Epoch 11 Batch 1100 Loss 2.2221\n",
      "Epoch 11 Batch 1200 Loss 2.0931\n",
      "Epoch 11 Batch 1300 Loss 0.0301\n",
      "Epoch 11 Batch 1400 Loss 0.0583\n",
      "Epoch 11 Batch 1500 Loss 1.1631\n",
      "Epoch 11 Batch 1600 Loss 0.5658\n",
      "Epoch 11 Batch 1700 Loss 1.8738\n",
      "Epoch 11 Batch 1800 Loss 3.3752\n",
      "Epoch 11 Batch 1900 Loss 0.8419\n",
      "Epoch 11 Batch 2000 Loss 0.3864\n",
      "Epoch 11 Batch 2100 Loss 3.5621\n",
      "Epoch 11 Batch 2200 Loss 1.6871\n",
      "Epoch 11 Batch 2300 Loss 0.4816\n",
      "Epoch 11 Batch 2400 Loss 0.9618\n",
      "Epoch 11 Batch 2500 Loss 1.7329\n",
      "Epoch 11 Batch 2600 Loss 0.6548\n",
      "Epoch 11 Batch 2700 Loss 2.7144\n",
      "Epoch 11 Batch 2800 Loss 2.1555\n",
      "Epoch 11 Batch 2900 Loss 1.6122\n",
      "Epoch 11 Batch 3000 Loss 1.6619\n",
      "Epoch 11 Batch 3100 Loss 1.0031\n",
      "Epoch 11 Batch 3200 Loss 1.2101\n",
      "Epoch 11 Batch 3300 Loss 2.7726\n",
      "Epoch 11 Batch 3400 Loss 2.3690\n",
      "Epoch 11 Batch 3500 Loss 1.5933\n",
      "Epoch 11 Batch 3600 Loss 3.6715\n",
      "Epoch 11 Batch 3700 Loss 0.4146\n",
      "Epoch 11 Batch 3800 Loss 2.3368\n",
      "Epoch 11 Batch 3900 Loss 0.9997\n",
      "Epoch 11 Batch 4000 Loss 1.6640\n",
      "Epoch 11 Batch 4100 Loss 1.4710\n",
      "Epoch 11 Batch 4200 Loss 0.9674\n",
      "Epoch 11 Batch 4300 Loss 0.0540\n",
      "Epoch 11 Batch 4400 Loss 0.6836\n",
      "Epoch 11 Batch 4500 Loss 2.6436\n",
      "Epoch 11 Batch 4600 Loss 0.0469\n",
      "Epoch 11 Batch 4700 Loss 1.2103\n",
      "Epoch 11 Batch 4800 Loss 0.1591\n",
      "Epoch 11 Batch 4900 Loss 1.6145\n",
      "Epoch 11 Batch 5000 Loss 0.2285\n",
      "Epoch 11 Batch 5100 Loss 1.4038\n",
      "Epoch 11 Batch 5200 Loss 0.4452\n",
      "Epoch 11 Batch 5300 Loss 0.0442\n",
      "Epoch 11 Batch 5400 Loss 1.6372\n",
      "Epoch 11 Batch 5500 Loss 3.4384\n",
      "Epoch 11 Batch 5600 Loss 2.2669\n",
      "Epoch 11 Batch 5700 Loss 0.2905\n",
      "Epoch 11 Batch 5800 Loss 3.0479\n",
      "Epoch 11 Batch 5900 Loss 1.8553\n",
      "Epoch 11 Batch 6000 Loss 0.1630\n",
      "Epoch 11 Batch 6100 Loss 2.7690\n",
      "Epoch 11 Batch 6200 Loss 1.0711\n",
      "Epoch 11 Batch 6300 Loss 1.1197\n",
      "Epoch 11 Batch 6400 Loss 1.0913\n",
      "Epoch 11 Batch 6500 Loss 1.3968\n",
      "Epoch 11 Batch 6600 Loss 3.7354\n",
      "Epoch 11 Batch 6700 Loss 1.8567\n",
      "Epoch 11 Batch 6800 Loss 1.7486\n",
      "Epoch 11 Batch 6900 Loss 1.0146\n",
      "Epoch 11 Batch 7000 Loss 2.3258\n",
      "Epoch 11 Batch 7100 Loss 1.5654\n",
      "Epoch 11 Batch 7200 Loss 1.8990\n",
      "Epoch 11 Batch 7300 Loss 4.1871\n",
      "Epoch 11 Batch 7400 Loss 0.5172\n",
      "Epoch 11 Batch 7500 Loss 2.8152\n",
      "Epoch 11 Batch 7600 Loss 0.7718\n",
      "Epoch 11 Batch 7700 Loss 2.5782\n",
      "Epoch 11 Batch 7800 Loss 0.2048\n",
      "Epoch 11 Batch 7900 Loss 1.6730\n",
      "Epoch 11 Batch 8000 Loss 1.7268\n",
      "Epoch 11 Batch 8100 Loss 0.7817\n",
      "Epoch 11 Batch 8200 Loss 0.0219\n",
      "Epoch 11 Batch 8300 Loss 1.0198\n",
      "Epoch 11 Batch 8400 Loss 2.9568\n",
      "Epoch 11 Batch 8500 Loss 0.6029\n",
      "Epoch 11 Batch 8600 Loss 0.7555\n",
      "Epoch 11 Batch 8700 Loss 0.9429\n",
      "Epoch 11 Batch 8800 Loss 3.0707\n",
      "Epoch 11 Batch 8900 Loss 0.5994\n",
      "Epoch 11 Batch 9000 Loss 0.2935\n",
      "Epoch 11 Batch 9100 Loss 2.4147\n",
      "Epoch 11 Batch 9200 Loss 3.7313\n",
      "Epoch 11 Batch 9300 Loss 2.8113\n",
      "Epoch 11 Batch 9400 Loss 2.0135\n",
      "Epoch 11 Batch 9500 Loss 0.2652\n",
      "Epoch 11 Batch 9600 Loss 4.2105\n",
      "Epoch 11 Batch 9700 Loss 4.1291\n",
      "Epoch 11 Batch 9800 Loss 0.4209\n",
      "Epoch 11 Batch 9900 Loss 0.9074\n",
      "Epoch 11 Batch 10000 Loss 2.3463\n",
      "Epoch 11 Batch 10100 Loss 4.3297\n",
      "Epoch 11 Batch 10200 Loss 2.9105\n",
      "Epoch 11 Batch 10300 Loss 4.0364\n",
      "Epoch 11 Batch 10400 Loss 0.8020\n",
      "Epoch 11 Batch 10500 Loss 1.6078\n",
      "Epoch 11 Batch 10600 Loss 2.7995\n",
      "Epoch 11 Batch 10700 Loss 0.0262\n",
      "Epoch 11 Batch 10800 Loss 2.2502\n",
      "Epoch 11 Batch 10900 Loss 3.8306\n",
      "Epoch 11 Batch 11000 Loss 0.5847\n",
      "Epoch 11 Batch 11100 Loss 3.0550\n",
      "Epoch 11 Batch 11200 Loss 1.9812\n",
      "Epoch 11 Batch 11300 Loss 0.3760\n",
      "Epoch 11 Batch 11400 Loss 4.2144\n",
      "Epoch 11 Batch 11500 Loss 1.2319\n",
      "Epoch 11 Batch 11600 Loss 2.3689\n",
      "Epoch 11 Batch 11700 Loss 5.3860\n",
      "Epoch 11 Batch 11800 Loss 4.7706\n",
      "Epoch 11 Batch 11900 Loss 3.2239\n",
      "Epoch 11 Batch 12000 Loss 1.3545\n",
      "Epoch 11 Batch 12100 Loss 0.1862\n",
      "Epoch 11 Batch 12200 Loss 2.7611\n",
      "Epoch 11 Batch 12300 Loss 1.5337\n",
      "Epoch 11 Batch 12400 Loss 0.3851\n",
      "Epoch 11 Batch 12500 Loss 0.0215\n",
      "Epoch 11 Batch 12600 Loss 1.1355\n",
      "Epoch 11 Batch 12700 Loss 1.2511\n",
      "Epoch 11 Batch 12800 Loss 1.9210\n",
      "Epoch 11 Batch 12900 Loss 0.0434\n",
      "Epoch 11 Batch 13000 Loss 2.1828\n",
      "Epoch 11 Batch 13100 Loss 3.2263\n",
      "Epoch 11 Batch 13200 Loss 0.0202\n",
      "Epoch 11 Batch 13300 Loss 3.2311\n",
      "Epoch 11 Batch 13400 Loss 0.9898\n",
      "Epoch 11 Batch 13500 Loss 0.3317\n",
      "Epoch 11 Batch 13600 Loss 1.9450\n",
      "Epoch 11 Batch 13700 Loss 3.6469\n",
      "Epoch 11 Batch 13800 Loss 1.1796\n",
      "Epoch 11 Batch 13900 Loss 1.5140\n",
      "Epoch 11 Batch 14000 Loss 0.3715\n",
      "Epoch 11 Batch 14100 Loss 1.1770\n",
      "Epoch 11 Batch 14200 Loss 3.3686\n",
      "Epoch 11 Batch 14300 Loss 1.2025\n",
      "Epoch 11 Batch 14400 Loss 0.5830\n",
      "Epoch 11 Batch 14500 Loss 1.3920\n",
      "Epoch 11 Batch 14600 Loss 1.1190\n",
      "Epoch 11 Batch 14700 Loss 2.4646\n",
      "Epoch 11 Batch 14800 Loss 3.1423\n",
      "Epoch 11 Batch 14900 Loss 1.3688\n",
      "Epoch 11 Batch 15000 Loss 3.0191\n",
      "Epoch 11 Batch 15100 Loss 1.6703\n",
      "Epoch 11 Batch 15200 Loss 0.7965\n",
      "Epoch 11 Batch 15300 Loss 2.7274\n",
      "Epoch 11 Batch 15400 Loss 2.7735\n",
      "Epoch 11 Batch 15500 Loss 0.0036\n",
      "Epoch 11 Batch 15600 Loss 0.0449\n",
      "Epoch 11 Batch 15700 Loss 1.2742\n",
      "Epoch 11 Batch 15800 Loss 3.9306\n",
      "Epoch 11 Batch 15900 Loss 0.8994\n",
      "Epoch 11 Batch 16000 Loss 1.8706\n",
      "Epoch 11 Batch 16100 Loss 1.9599\n",
      "Epoch 11 Batch 16200 Loss 0.9215\n",
      "Epoch 11 Batch 16300 Loss 2.4205\n",
      "Epoch 11 Batch 16400 Loss 1.9024\n",
      "Epoch 11 Batch 16500 Loss 0.5142\n",
      "Epoch 11 Batch 16600 Loss 3.9679\n",
      "Epoch 11 Batch 16700 Loss 1.0785\n",
      "Epoch 11 Batch 16800 Loss 3.8624\n",
      "Epoch 11 Batch 16900 Loss 2.7431\n",
      "Epoch 11 Batch 17000 Loss 0.8389\n",
      "Epoch 11 Batch 17100 Loss 2.3840\n",
      "Epoch 11 Loss 1.7509\n",
      "Time taken for 1 epoch 967.859325170517 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.3322\n",
      "Epoch 12 Batch 100 Loss 0.6598\n",
      "Epoch 12 Batch 200 Loss 1.3709\n",
      "Epoch 12 Batch 300 Loss 0.6841\n",
      "Epoch 12 Batch 400 Loss 3.0927\n"
     ]
    }
   ],
   "source": [
    "#Only RUN FOR TRAINING\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "import time\n",
    "EPOCHS = 500\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a2e9cc-d01a-48fb-ac8a-ab4528077ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15c6c75-2fed-41b8-8143-25b2e1a1fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "\n",
    "train = pd.read_csv(r'Data/train_final.csv')\n",
    "dev = pd.read_csv(r'Data/dev_final.csv')\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z0-9?.!,¿|]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "# Convert sequences to tokenizers\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    # tensor = tf.ragged.constant(tensor)\n",
    "    # print(tensor)\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(number):\n",
    "    # Create dataset (targ_lan = English, inp_lang = French)\n",
    "    inp_lang = (train['paragraph']+\" | \"+train['question']).copy()\n",
    "    targ_lang = (train['answer']).copy()\n",
    "    act_inp = []\n",
    "    act_out = []\n",
    "    ep = number\n",
    "    if number == 0:\n",
    "        ep = len(inp_lang)\n",
    "    for i in range(ep):\n",
    "        if(len(str(targ_lang[i]).split(' '))==1):\n",
    "            act_inp.append(preprocess_sentence(str(inp_lang[i])))\n",
    "            act_out.append(preprocess_sentence(str(targ_lang[i])))\n",
    "    \n",
    "    inp_lang = (dev['paragraph']+\" | \"+dev['question']).copy()\n",
    "    targ_lang = (dev['answer']).copy()\n",
    "    if number == 0:\n",
    "        ep = len(inp_lang)\n",
    "    for i in range(ep):\n",
    "        if(len(str(targ_lang[i]).split(' '))==1):\n",
    "            act_inp.append(preprocess_sentence(str(inp_lang[i])))\n",
    "            act_out.append(preprocess_sentence(str(targ_lang[i])))\n",
    "            \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(act_inp)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(act_out)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb164a16-3e1a-40d6-95bf-5f924c3eb32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 579\n",
      "17116 17116 4280 4280\n"
     ]
    }
   ],
   "source": [
    "number_of_instances = 0\n",
    "\n",
    "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(number_of_instances)\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "print(max_length_targ, max_length_inp )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.42B.300d.txt', encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "inp_embedding_matrix = np.zeros((len(inp_lang_tokenizer.word_index) + 1, 300))\n",
    "targ_embedding_matrix = np.zeros((len(targ_lang_tokenizer.word_index) + 1, 300))\n",
    "\n",
    "for word, i in inp_lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        inp_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "for word, i in targ_lang_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        targ_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fd4434-53fb-4386-8f46-adaa8928b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 50\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8080f7-70e4-4b2e-9fe3-c6bc05dea22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 579) (1, 3)\n",
      "Encoder output shape: (batch size, sequence length, units) (1, 579, 50)\n",
      "Encoder Hidden state shape: (batch size, units) (1, 50)\n",
      "Attention result shape: (batch size, units) (1, 50)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (1, 579, 1)\n",
      "Decoder output shape: (batch_size, vocab size) (1, 9720)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# Size of input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "print(example_input_batch.shape, example_target_batch.shape)\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,  weights=[inp_embedding_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "\n",
    "# Attention Mechanism\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n",
    "\n",
    "# Decoder class\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[targ_embedding_matrix], trainable=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "\n",
    "\n",
    "# Initialize optimizer and loss functions\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction='none')\n",
    "\n",
    "# Loss function\n",
    "def loss_function(real, pred):\n",
    "    # Take care of the padding. Not all sequences are of equal length.\n",
    "    # If there's a '0' in the sequence, the loss is being nullified\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    return\n",
    "\n",
    "def get_data(element):\n",
    "    question = ''\n",
    "    answer = ''\n",
    "    for k in element[0].numpy():\n",
    "        if k!=0:\n",
    "            question = question+ inp_lang_tokenizer.index_word[k] + ' '\n",
    "    for k in element[1].numpy():\n",
    "        if k!=0:\n",
    "            answer = answer+ targ_lang_tokenizer.index_word[k] + ' '\n",
    "    return question, answer\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "checkpoint_dir = './training_checkpoints1'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: \n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        return (int(gold_toks == pred_toks),1,1)\n",
    "    if num_same == 0:\n",
    "        return 0,0,0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return (f1, precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8922e61-6f36-437e-9692-4d80b7ceb3d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores of who what when where\n",
      "0.165625,0.2372775522947237,0.48633879781420764,0.22115384615384615,0.2630841121495327\n",
      "Similarity Scores\n",
      "0.326566963710235,0.3969360508186101,0.6775109335356773,0.36664117166365673,0.4261921648075726\n",
      "Precisions\n",
      "0.171875,0.2379019669060256,0.48816029143898,0.22115384615384615,0.26425233644859814\n",
      "Recalls\n",
      "0.171875,0.2379019669060256,0.48816029143898,0.22115384615384615,0.26425233644859814\n"
     ]
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "score_who = 0\n",
    "score_what = 0\n",
    "score_when = 0\n",
    "score_where = 0\n",
    "s_who = 0\n",
    "s_what = 0\n",
    "s_when = 0\n",
    "s_where = 0\n",
    "pre_who = 0\n",
    "pre_what = 0\n",
    "pre_when = 0\n",
    "pre_where = 0\n",
    "rec_who = 0\n",
    "rec_what = 0\n",
    "rec_when = 0\n",
    "rec_where = 0\n",
    "count_who = 0\n",
    "count_what = 0\n",
    "count_when = 0\n",
    "count_where = 0\n",
    "\n",
    "def isWordPresent(sentence, word):\n",
    "    s = sentence.split(\" \")\n",
    "    for i in s:\n",
    "        if (i.lower() == word):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for element in dataset_test:\n",
    "    question, answer = get_data(element)\n",
    "    question = question[7:-6]\n",
    "    idx = question.index('|')\n",
    "    ques = question[idx+2:]\n",
    "    answer = answer[7:-6]\n",
    "    pred_ans = evaluate(question)[0][:-6]\n",
    "    if isWordPresent(ques, 'who'):\n",
    "        f,p,r = compute_f1(answer, pred_ans)\n",
    "        s_who = s_who + similar(answer, pred_ans)\n",
    "        score_who = score_who + f\n",
    "        rec_who = rec_who + r\n",
    "        pre_who = pre_who + p\n",
    "        count_who = count_who +1\n",
    "    elif isWordPresent(ques, 'what'):\n",
    "        f,p,r = compute_f1(answer, pred_ans)\n",
    "        s_what = s_what + similar(answer, pred_ans)\n",
    "        score_what = score_what + f\n",
    "        rec_what = rec_what + r\n",
    "        pre_what = pre_what + p\n",
    "        count_what = count_what +1\n",
    "    elif isWordPresent(ques, 'when'):\n",
    "        f,p,r = compute_f1(answer, pred_ans)\n",
    "        s_when = s_when + similar(answer, pred_ans)\n",
    "        score_when = score_when + f\n",
    "        rec_when = rec_when + r\n",
    "        pre_when = pre_when + p\n",
    "        count_when = count_when +1\n",
    "    elif isWordPresent(ques, 'where'):\n",
    "        f,p,r = compute_f1(answer, pred_ans)\n",
    "        s_where = s_where + similar(answer, pred_ans)\n",
    "        score_where = score_where + f\n",
    "        rec_where = rec_where + r\n",
    "        pre_where = pre_where + p\n",
    "        count_where = count_where +1\n",
    "\n",
    "print(\"F1 Scores of who what when where\")        \n",
    "print(score_who/count_who, end =',')\n",
    "print(score_what/count_what, end =',')\n",
    "print(score_when/count_when, end =',')\n",
    "print(score_where/count_where, end =',')\n",
    "print((score_who+score_what+score_when+score_where)/(count_who+count_what+count_when+count_where))\n",
    "\n",
    "print(\"Similarity Scores\")\n",
    "print(s_who/count_who, end =',')\n",
    "print(s_what/count_what, end =',')\n",
    "print(s_when/count_when, end =',')\n",
    "print(s_where/count_where, end =',')\n",
    "print((s_who+s_what+s_when+s_where)/(count_who+count_what+count_when+count_where))\n",
    "\n",
    "print(\"Precisions\")\n",
    "print(pre_who/count_who, end =',')\n",
    "print(pre_what/count_what, end =',')\n",
    "print(pre_when/count_when, end =',')\n",
    "print(pre_where/count_where, end =',')\n",
    "print((pre_who+pre_what+pre_when+pre_where)/(count_who+count_what+count_when+count_where))\n",
    "\n",
    "print(\"Recalls\")\n",
    "print(rec_who/count_who, end =',')\n",
    "print(rec_what/count_what, end =',')\n",
    "print(rec_when/count_when, end =',')\n",
    "print(rec_where/count_where, end =',')\n",
    "print((rec_who+rec_what+rec_when+rec_where)/(count_who+count_what+count_when+count_where))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8078918-d863-4860-a795-a070c2834aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7171c84-2314-42f8-b54c-ad2ca8f3d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\"some normans joined turkish forces to aid in the destruction of the armenians vassal states of sassoun and taron in far eastern anatolia later many took up service with the armenian state further south in cilicia and the taurus mountains a norman named oursel led a force of franks into the upper euphrates valley in northern syria from 1073 to 1074 8 000 of the 20 000 troops of the armenian general philaretus brachamius were normans formerly of oursel led by raimbaud they even lent their ethnicity to the name of their castle afranji meaning franks the known trade between amalfi and antioch and between bari and tarsus may be related to the presence of italo normans in those cities while amalfi and bari were under norman rule in italy | who was the leader when the franks entered the euphrates valley\")[0][:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d0134-0c3f-46b4-9dd3-277dc8776811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
